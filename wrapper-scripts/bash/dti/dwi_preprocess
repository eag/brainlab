#!/bin/bash

### NOTES ###
# - this script performs standard DTI preprocessing steps suitable for clinical data
# - EPI distortion correction is performed only if RPE data was acquired
# - expects input as the subject folder containing DTI data from DICOM conversion
#   - i.e. output from dcm2niix or equivalent (nii.gz, .bval, .bvec, and .json files)
# - the input folder must contain either:
#     1. a single DTI acquisition, or
#     2. a DTI acquisition uniquely identified by the prefix <dwi_pfx*>
# - does not explicitly verify that the input data is a DTI scan, otherwise will simply attempt 
#   to process and fail
# - processing steps determined primarily from image and json sidecar data found in
#   the input folder and specifics are written to the log file

# TODO: 
# - currently assumes "b0_1.nii.gz" and "b0_2.nii.gz" suffix for dual b0 acquisitions
# - dual full DTI acquisitions not yet tested/implemented

#===============================================================================
### PARSE ARGUMENTS AND SET GLOBAL VARIABLES ###
#===============================================================================

parse_arguments() 
{
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --in_dir|--out_dir|--scratch_dir|--alps_roi_dir|--threads|--dry_run|--sub_id|--dwi_pfx|--regrid_size)
        flag=$(echo "${1#--}" | tr '[:lower:]' '[:upper:]')  # e.g., in_dir -> IN_DIR
        value="$2"
        if [[ -z "$value" || "$value" == --* ]]; then
          echo "ERROR: Missing value for $1" >&2
          exit 1
        fi
        declare -g "${flag}=$value"  # sets global variable like IN_DIR="/path"
        shift 2
        ;;
      -h|--help)
        echo "Usage: $0 --in_dir DIR --out_dir DIR --scratch_dir DIR --alps_roi_dir DIR"
        echo "          --threads N --dry_run 0|1 --sub_id ID --dwi_pfx PFX --regrid_size X,Y,Z"
        echo -e "Notes\n  PFX=\"*\" for folders containing a single DTI acquisition"
        exit 0
        ;;
      *)
        echo "ERROR: Unknown argument: $1" >&2
        exit 1
        ;;
    esac
  done

  ### confirm all required global variables are set ###
  required_vars=(
    IN_DIR OUT_DIR SCRATCH_DIR ALPS_ROI_DIR
    THREADS DRY_RUN SUB_ID DWI_PFX REGRID_SIZE
  )
  for var in "${required_vars[@]}"; do
    if [[ -z "${!var}" ]]; then
      echo "ERROR: Missing required argument --$(echo "$var" | tr '[:upper:]' '[:lower:]')" >&2
      exit 1
    fi
  done

  ### set all other global variables ###
  OUT_BASE_DIR="${OUT_DIR}/${SUB_ID}"
  QC_DIR="${OUT_DIR}/qc"
  LOG_DIR="${OUT_DIR}/logs"
  LOG_FN=${LOG_DIR}/${SUB_ID}_dwi_preprocess.log
  REGRID_SIZE_FORMATTED=$(echo "${REGRID_SIZE}" | sed 's/,/_/g')
}

#-------------------------------------------------------------------------------
### GLOBAL VARIABLES (auto-filled later) ###
#-------------------------------------------------------------------------------

PRIMARY_DTI_NII=""
PRIMARY_DTI_BVAL=""
PRIMARY_DTI_BVEC=""
PRIMARY_DTI_JSON=""
RPE_DWI_NII=""
RPE_BVAL=""
RPE_BVEC=""
RPE_JSON=""
RPE_B0_1=""
RPE_B0_2=""
RPE_B0_1_JSON=""
RPE_B0_2_JSON=""
RPE_MIF=""
SLICE_OR=""

#-------------------------------------------------------------------------------
### HELPER FUNCTIONS ###
#-------------------------------------------------------------------------------

log() 
{ 
  echo "[$(date '+%F %T')] $*" | tee -a "${LOG_FN}"; 
}

setup_environment()
{
  if [[ -n ${CC_CLUSTER} ]]; then
    log "Running on CC cluster: ${CC_CLUSTER}"
    module load ants/2.5.0
    module load mrtrix/3.0.4
    module load fsl/6.0.7.7
  else
    ### TODO: update for local environment ###
    export ANTSPATH=${HOME}/work/code/external-tools/ants/build/ANTS-build/Examples
    export PATH=${PATH}:${ANTSPATH}
  fi
}

setup_directories()
{
  ### DRY_RUN existence of required input directories ###
  declare -A REQUIRED_DIRS=(
    ["Input directory"]="${IN_DIR}/${SUB_ID}"
    ["Scratch directory"]="${SCRATCH_DIR}"
    ["ALPS ROI directory"]="${ALPS_ROI_DIR}"
  )

  for label in "${!REQUIRED_DIRS[@]}"; do
    dir="${REQUIRED_DIRS[$label]}"
    if [[ ! -d "${dir}" ]]; then
      echo "ERROR: ${label} does not exist: ${dir}" >&2
      exit 1
    fi
  done

  ### create output directories  ###
  for dir in "${OUT_BASE_DIR}" "${QC_DIR}" "${LOG_DIR}"; do
    mkdir -p "${dir}" || 
    {
      echo "ERROR: Failed to create output directory: ${dir}" >&2
      exit 1
    }
  done

  ### DRY_RUN writability of output directories ###
  declare -A WRITABLE_DIRS=(
    ["Scratch directory"]="${SCRATCH_DIR}"
    ["Output base directory"]="${OUT_BASE_DIR}"
  )

  for label in "${!WRITABLE_DIRS[@]}"; do
    dir="${WRITABLE_DIRS[$label]}"
    if [[ ! -w "${dir}" ]]; then
      echo "ERROR: ${label} not writable: ${dir}" >&2
      exit 1
    fi
  done
}

run_if_missing() 
{
  local outfile="$1"
  shift
  if [[ ! -e "${outfile}" ]]; then
    log "Running: $*"
    "$@"
  else
    log "$(basename "${outfile}") already exists, skipping"
  fi
}

run_if_exists() 
{
  local outfile="$1"
  shift
  if [[ -e "${outfile}" ]]; then
    log "Running: $*"
    "$@"
  fi
}

setup_output_directories()
{
  OUT_BASE_DIR="${OUT_DIR}/${SUB_ID}"
  QC_DIR="${OUT_DIR}/qc"
  LOG_DIR="${OUT_DIR}/logs"
  LOG_FN="${LOG_DIR}/${SUB_ID}_dwi_preprocess.log"

  mkdir -p "${OUT_BASE_DIR}" "${QC_DIR}" "${LOG_DIR}"
  : > "${LOG_FN}" 
}

check_exit() 
{
  local status=$1
  local msg=$2
  if [[ $status -ne 0 ]]; then
    log "ERROR: $msg"
    exit $status
  fi
}

#-------------------------------------------------------------------------------
### CONFIG FUNCTIONS ###
#-------------------------------------------------------------------------------

detect_dti_files() {
  pushd "${IN_DIR}/${SUB_ID}" > /dev/null

  ### exit if more than two full DTI acquisitions are found ###
  dti_files=(${DWI_PFX}*.bvec)
  if (( ${#dti_files[@]} > 2 )); then
    log "ERROR: More than two DTI .nii.gz files found â€” unknown scenario"
    log "Please DRY_RUN input directory: ${IN_DIR}/${SUB_ID}/${DWI_PFX}*.nii.gz"
    exit 1
  fi

  ### first look for b0_1 and b0_2 ###
  RPE_B0_1=$(ls *b0_1*.nii.gz 2>/dev/null | head -n1)
  RPE_B0_2=$(ls *b0_2*.nii.gz 2>/dev/null | head -n1)
  if [[ -n "${RPE_B0_1}" && -n "${RPE_B0_2}" ]]; then
    RPE_B0_1_JSON="${RPE_B0_1%.nii.gz}.json"
    RPE_B0_2_JSON="${RPE_B0_2%.nii.gz}.json"
    if [[ -f "${RPE_B0_1}" && -f "${RPE_B0_1_JSON}" && -f "${RPE_B0_2}" && -f "${RPE_B0_2_JSON}" ]]; then
      local pe_b0_1=$(jq -r '.PhaseEncodingDirection' "${RPE_B0_1_JSON}")
      local pe_b0_2=$(jq -r '.PhaseEncodingDirection' "${RPE_B0_2_JSON}")
      log "RPE b0_1 PE: ${pe_b0_1}"
      log "RPE b0_2 PE: ${pe_b0_2}"
      if [[ "${pe_b0_1}" != "${pe_b0_2}" ]]; then
        log "ERROR: RPE b0_1 and b0_2 have different PhaseEncodingDirection"
        exit 1
      fi
      log "Found two RPE b0 images, generating combined rpe.mif"
      RPE_MIF="${OUT_DIR}/${SUB_ID}/dwi_detected_combined_rpe.mif"
      run_if_missing "${RPE_MIF}" \
        run_create_combined_rpe "${RPE_B0_1}" "${RPE_B0_1_JSON}" "${RPE_B0_2}" "${RPE_B0_2_JSON}" "${RPE_MIF}"
    fi
  fi

  ### then look for primary DTI scan ###
  for nii in ${DWI_PFX}*.nii.gz; do
    base="${nii%.nii.gz}"
    bval="${base}.bval"
    bvec="${base}.bvec"
    json="${base}.json"
    if [[ -f "${nii}" && -f "${bval}" && -f "${bvec}" && -f "${json}" ]]; then
      if ! jq -e 'has("PhaseEncodingDirection")' "$json" > /dev/null; then
        log "ERROR: PhaseEncodingDirection missing in ${json}"
        log "Please add first, inferring from PhaseEncodingAxis + ImageOrientationPatientDICOM"
        exit 1
      fi
      PRIMARY_DTI_NII="${nii}"
      PRIMARY_DTI_BVAL="${bval}"
      PRIMARY_DTI_BVEC="${bvec}"
      PRIMARY_DTI_JSON="${json}"
      break  # stop searching after finding the first DTI scan
    fi
  done

  ### if no rpe.mif (from b0_1/b0_2), look for second full DTI scan ###
  # TODO: implement
  if [[ -z "${RPE_MIF}" ]]; then
    for nii in ${DWI_PFX}*.nii.gz; do
      [[ "$nii" == "${PRIMARY_DTI_NII}" ]] && continue
      base="${nii%.nii.gz}"
      bval="${base}.bval"
      bvec="${base}.bvec"
      json="${base}.json"
      if [[ -f "${nii}" && -f "${bval}" && -f "${bvec}" && -f "${json}" ]]; then
        log "Found second DTI scan: ERROR -- not yet implemented"
        exit 1
      fi
    done
  fi

  ### if still no rpe.mif, look for single RPE volume with json only ###
  if [[ -z "${RPE_MIF}" ]]; then
    for nii in *.nii.gz; do
      [[ "$nii" == "${PRIMARY_DTI_NII}" ]] && continue
      base="${nii%.nii.gz}"
      bval="${base}.bval"
      bvec="${base}.bvec"
      json="${base}.json"
      if [[ -f "${json}" && ! -f "${bval}" && ! -f "${bvec}" ]]; then
        log "Found single-volume B0"
        RPE_DWI_NII="${nii}"
        RPE_MIF="${OUT_DIR}/${SUB_ID}/dwi_detected_rpe.mif"
        run_create_rpe "${nii}" "${json}" "${RPE_MIF}"
        break
      fi
    done
  fi

  ### finalize full paths for the non-empty variables ###
  for var in PRIMARY_DTI_NII PRIMARY_DTI_BVAL PRIMARY_DTI_BVEC PRIMARY_DTI_JSON \
             RPE_B0_1 RPE_B0_2 RPE_B0_1_JSON RPE_B0_2_JSON; do
    val="${!var}"
    [[ -n "$val" ]] && eval "$var=\"${IN_DIR}/${SUB_ID}/$val\""
  done

  ### log whether RPE data was found ###
  [[ -n "${RPE_MIF}" ]] && log "RPE data: ${RPE_MIF}" || log "No RPE data found"

  popd > /dev/null
}

verify_minimum_required_inputs()
{
  log "PRIMARY_DTI_NII : $(basename "${PRIMARY_DTI_NII:-}")"
  log "PRIMARY_DTI_BVAL: $(basename "${PRIMARY_DTI_BVAL:-}")"
  log "PRIMARY_DTI_BVEC: $(basename "${PRIMARY_DTI_BVEC:-}")"
  log "PRIMARY_DTI_JSON: $(basename "${PRIMARY_DTI_JSON:-}")"
  log "RPE_DWI_NII    : $(basename "${RPE_DWI_NII:-}")"
  log "RPE_BVAL      : $(basename "${RPE_BVAL:-}")"
  log "RPE_BVEC      : $(basename "${RPE_BVEC:-}")"
  log "RPE_JSON      : $(basename "${RPE_JSON:-}")"
  log "RPE_B0_1      : $(basename "${RPE_B0_1:-}")"
  log "RPE_B0_2      : $(basename "${RPE_B0_2:-}")"
  log "RPE_B0_1_JSON  : $(basename "${RPE_B0_1_JSON:-}")"
  log "RPE_B0_2_JSON  : $(basename "${RPE_B0_2_JSON:-}")"
  log "RPE_MIF       : $(basename "${RPE_MIF:-}")"

  if [[ -z "${PRIMARY_DTI_NII}" || -z "${PRIMARY_DTI_BVAL}" || \
        -z "${PRIMARY_DTI_BVEC}" || -z "${PRIMARY_DTI_JSON}" ]]; then
    log "ERROR: required input [nii.gz + json + bvec + bval] not found"
    log "Please DRY_RUN input directory: ${IN_DIR}/${SUB_ID}/${DWI_PFX}*"
    exit 1
  fi
}

detect_slice_orientation_from_json() 
{
  local json_file="$1"

  if [[ ! -f "${json_file}" ]]; then
    return
  fi

  local vals
  vals=($(jq -r '.ImageOrientationPatientDICOM[]?' "${json_file}"))

  if [[ "${#vals[@]}" -ne 6 ]]; then
    return
  fi

  local Xx="${vals[0]}" Xy="${vals[1]}" Xz="${vals[2]}"
  local Yx="${vals[3]}" Yy="${vals[4]}" Yz="${vals[5]}"

  local Sx Sy Sz
  Sx=$(echo "${Xy} * ${Yz} - ${Xz} * ${Yy}" | bc -l)
  Sy=$(echo "${Xz} * ${Yx} - ${Xx} * ${Yz}" | bc -l)
  Sz=$(echo "${Xx} * ${Yy} - ${Xy} * ${Yx}" | bc -l)

  local abs_Sx abs_Sy abs_Sz
  abs_Sx=$(echo "${Sx#-}" | bc -l)
  abs_Sy=$(echo "${Sy#-}" | bc -l)
  abs_Sz=$(echo "${Sz#-}" | bc -l)

  if (( $(echo "${abs_Sz} >= ${abs_Sx} && ${abs_Sz} >= ${abs_Sy}" | bc -l) )); then
    SLICE_OR="0,1"  # axial
  elif (( $(echo "${abs_Sy} >= ${abs_Sx} && ${abs_Sy} >= ${abs_Sz}" | bc -l) )); then
    SLICE_OR="0,2"  # coronal
  else
    SLICE_OR="1,2" # sagittal
  fi
  log "Slice orientation detected: ${SLICE_OR}"

  if [[ "${SLICE_OR}" != "0,1" ]]; then
    log "WARNING: Slice orientation is not axial (0,1). Detected: ${SLICE_OR}"
  fi
}

#-------------------------------------------------------------------------------
### PREPROCESSING FUNCTIONS ###
#-------------------------------------------------------------------------------
# - "pipe_" : pipeline functions that operate on staged filenames
# - "run_"  : utility functions used by pipe functions with filenames as arguments

run_create_rpe()
{
  local nii="$1"
  local json="$2"
  local out_mif="$3"

  if [[ ${DRY_RUN} -eq 1 ]]; then
    log "Skipping RPE creation for test run"
    return
  fi

  run_if_missing "${out_mif}" \
    mrconvert "${nii}" "${out_mif}" -json_import "${json}" -quiet -force
  check_exit $? "mrconvert failed for ${nii}"
}

run_create_combined_rpe() {
  local b0_1="$1"
  local b0_1_json="$2"
  local b0_2="$3"
  local b0_2_json="$4"
  local out_mif="$5"

  if [[ ${DRY_RUN} -eq 1 ]]; then
    log "Skipping RPE creation for test run"
    return
  fi

  if [[ -f "${out_mif}" ]]; then
    log "RPE already exists, skipping creation"
    exit 0
  fi

  local tmp1=$(mktemp "$(pwd)/tmp_rpe_$(date +%Y%m%d)_XXXXXX.mif")
  local tmp2=$(mktemp "$(pwd)/tmp_rpe_$(date +%Y%m%d)_XXXXXX.mif")

  mrconvert "${b0_1}" "${tmp1}" -json_import "${b0_1_json}" -quiet -force
  mrconvert "${b0_2}" "${tmp2}" -json_import "${b0_2_json}" -quiet -force
  mrcat "${tmp1}" "${tmp2}" -axis 3 "${out_mif}" -quiet -force
  check_exit $? "create combined rpe failed"

  rm -f "${tmp1}" "${tmp2}"
}


run_dwidenoise() 
{
  local input="$1"
  local output="$2"
  run_if_missing "${output}" \
    dwidenoise "${input}" "${output}" -nthreads "${THREADS}"
  check_exit $? "dwidenoise failed for ${input}"
}

run_degibbs() 
{
  local input="$1"
  local output="$2"
  detect_slice_orientation_from_json "${PRIMARY_DTI_JSON}"
  
  if [[ -z "${SLICE_OR}" ]]; then
    log "Slice orientation not detected, using default (0,1)"
    SLICE_OR="0,1"
  fi
  
  run_if_missing "${output}" \
    mrdegibbs "${input}" "${output}" -nthreads "${THREADS}" -axes "${SLICE_OR}"
  check_exit $? "mrdegibbs failed for ${input}"
}

run_screen_grab_image()
{
  ### generate png of "input" with "output_pnf_pfx" in QC_DIR with optional "overlay" ###
  local input="$1"
  local output_png_pfx="$2"
  local overlay="$3"
 
  local gui_runner=""
  if [[ "${CC_CLUSTER}" == "cedar" ]]; then
    gui_runner="run-xvfb"
  elif [[ "${CC_CLUSTER}" == "narval" ]]; then
    gui_runner=xvfb-run
  fi

  local cmd="${gui_runner} mrview ${input}"
  if [[ -n "${overlay}" ]]; then
    cmd+=" -overlay.load ${overlay} -overlay.opacity 0.5 -overlay.colourmap 1"
  fi
  cmd+=" -mode 2 -capture.prefix ${output_png_pfx}_ -capture.folder ${QC_DIR} -capture.grab -exit"

  if [[ ! -e "${QC_DIR}/${output_png_pfx}_0000.png" && -e "${input}" && ( -z "${overlay}" || -e "${overlay}" ) ]]; then
    log "Running: ${cmd}"
    bash -c "${cmd}"
  else
    log "Skipping QC image generation: output already exists or input/overlay missing"
  fi
}

run_qc_residual() 
{
  ### generate residual image "output" from "original" and "processed" images ###
  ### save png of residual image with "png_pfx" in QC_DIR ###
  local original=$1
  local processed=$2
  local output=$3
  local png_pfx=$4

  ### generate residual image ###
  run_if_missing ${output} \
    mrcalc "${original}" "${processed}" -subtract ${output} -force -nthreads "${THREADS}"

  ### generate png of residual image ###
  run_screen_grab_image "${output}" "${png_pfx}"
}

pipe_mrconvert()
{
  run_if_missing "${OUT_BASE_DIR}/dwi.mif" \
    mrconvert ${PRIMARY_DTI_NII} "${OUT_BASE_DIR}/dwi.mif" \
      -fslgrad ${PRIMARY_DTI_BVEC} ${PRIMARY_DTI_BVAL} \
      -json_import ${PRIMARY_DTI_JSON} \
      -nthreads "${THREADS}"
   check_exit $? "mrconvert failed for ${PRIMARY_DTI_NII}"
}

run_quantatitive_eddy_qc()
{
  local eddy_dir="${OUT_BASE_DIR}/eddyqc_dir"
  local input_dwi="${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy.mif"
  local output_csv="${QC_DIR}/${SUB_ID}_eddy_qc_quantitative.csv"

  local file_movement_rms="${eddy_dir}/eddy_movement_rms"
  local file_restricted_movement_rms="${eddy_dir}/eddy_restricted_movement_rms"
  local file_outlier_map="${eddy_dir}/eddy_outlier_map"

  ### extract mean absolute motion (second column) ###
  mean_abs_motion=$(awk '{sum+=$2} END {if (NR>0) print sum/NR; else print "NA"}' "$file_movement_rms")

  ### extract mean relative motion (second column) ###
  mean_rel_motion=$(awk '{sum+=$2} END {if (NR>0) print sum/NR; else print "NA"}' "$file_restricted_movement_rms")

  ### calculate percent outlier slices ###
  n_outlier_slices=$(wc -l < "$file_outlier_map")

  n_volumes=$(mrinfo "$input_dwi" -size | awk '{print $4}')
  n_slices=$(mrinfo "$input_dwi" -size | awk '{print $3}')
  total_slices=$(( n_volumes * n_slices ))

  if [[ $total_slices -gt 0 ]]; then
    percent_outliers=$(awk -v outliers="$n_outlier_slices" -v total="$total_slices" 'BEGIN {print 100*outliers/total}')
  else
    percent_outliers="NA"
  fi

  ### write output CSV ###
  {
    echo "MeanAbsMotion,MeanRelMotion,PercentOutliers"
    echo "${mean_abs_motion},${mean_rel_motion},${percent_outliers}"
  } > "$output_csv"
}

pipe_dwi_preprocessing()
{
  log "preprocessing DWI"

  run_dwidenoise ${OUT_BASE_DIR}/dwi.mif \
    ${OUT_BASE_DIR}/dwi_denoised.mif
  
  run_degibbs ${OUT_BASE_DIR}/dwi_denoised.mif \
    ${OUT_BASE_DIR}/dwi_denoised_degibbs.mif

  run_qc_residual ${OUT_BASE_DIR}/dwi.mif \
    ${OUT_BASE_DIR}/dwi_denoised.mif \
    ${OUT_BASE_DIR}/dwi_denoised_residuals.mif \
    ${SUB_ID}_dwi_denoised_residuals
    
  run_qc_residual ${OUT_BASE_DIR}/dwi_denoised.mif \
    ${OUT_BASE_DIR}/dwi_denoised_degibbs.mif \
    ${OUT_BASE_DIR}/dwi_denoised_degibbs_residuals.mif \
    ${SUB_ID}_dwi_denoised_degibbs_residuals
}

pipe_rpe_preprocessing()
{
  ### preprocess rpe, if available (scenario 2) ###
  if [[ -f "${RPE_MIF}" && -z "${RPE_BVEC}" ]]; then
    log "preprocessing RPE B0"
    log "denoising set to null operation"
    run_degibbs ${RPE_MIF} \
      "${OUT_BASE_DIR}/rpe_denoised_degibbs.mif"

    run_qc_residual ${RPE_MIF} \
      ${OUT_BASE_DIR}/rpe_denoised_degibbs.mif \
      ${OUT_BASE_DIR}/rpe_denoised_degibbs_residuals.mif \
      ${SUB_ID}_rpe_denoised_degibbs_residuals
  fi

  ### TODO: implement scenario 3 ###

}

pipe_dwipreproc()
{
  local input=${OUT_BASE_DIR}/dwi_denoised_degibbs.mif
  local output=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy.mif
  local scratch_dir=${SCRATCH_DIR}/tmp_${SUB_ID}_$(date +%Y%m%d_%H%M%S)

  ### run dwifslpreproc by scenario ###
  if [[ -f "${RPE_MIF}" && -n "${RPE_BVEC}" ]]; then
  
    # scenario 3
    log "TODO: implement scenario 3 -- scenario 2 should work"
    exit 1

  elif [[ -f "${RPE_MIF}" && -z "${RPE_BVEC}" && -z ${RPE_B0_1} ]]; then
    # scenario 2a
    log "Running dwifslpreproc for DWI RPE (scenario 2a)"
    local rpe=${OUT_BASE_DIR}/rpe_denoised_degibbs.mif
    run_if_missing "${output}" \
      dwifslpreproc ${input} ${output} \
        -rpe_header \
        -se_epi ${rpe} -align_seepi \
        -eddy_options "--slm=linear --repol --data_is_shelled --verbose --estimate_move_by_susceptibility" \
        -eddyqc_all ${OUT_BASE_DIR}/eddyqc_dir \
        -info \
        -scratch ${scratch_dir} \
        -nthreads "${THREADS}" \
        -force
    check_exit $? "dwifslpreproc failed for ${input}"

  elif [[ -f "${RPE_MIF}" && -z "${RPE_BVEC}" && -n "${RPE_B0_1}" ]]; then
    # scenario 2b
    log "Running dwifslpreproc for DWI RPE-PAIR (scenario 2b)"
    local rpe=${OUT_BASE_DIR}/rpe_denoised_degibbs.mif
    local pe=$(jq -r '.PhaseEncodingDirection' "${RPE_B0_1_JSON}")
    run_if_missing "${output}" \
      dwifslpreproc ${input} ${output} \
        -rpe_pair -pe_dir=${pe} \
        -se_epi ${rpe} -align_seepi \
        -eddy_options "--slm=linear --repol --data_is_shelled --verbose --estimate_move_by_susceptibility" \
        -eddyqc_all ${OUT_BASE_DIR}/eddyqc_dir \
        -info \
        -scratch ${scratch_dir} \
        -nthreads "${THREADS}" \
        -force
    check_exit $? "dwifslpreproc failed for ${input}"
    
  else
    # scenario 1
    log "Running dwifslpreproc for DWI no RPE acquisition (scenario 1)"
    run_if_missing ${output} \
      dwifslpreproc ${input} ${output} \
        -rpe_header \
        -eddy_options "--slm=linear --repol --data_is_shelled --verbose" \
        -eddyqc_all ${OUT_BASE_DIR}/eddyqc_dir \
        -info \
        -scratch ${scratch_dir} \
        -nthreads "${THREADS}" \
        -force
    check_exit $? "dwifslpreproc failed for ${input}"
  fi

  ### qc dwifslpreproc ###
  cp ${OUT_BASE_DIR}/eddyqc_dir/quad/qc.pdf ${QC_DIR}/${SUB_ID}_quad_eddy_qc.pdf
  run_quantatitive_eddy_qc
  run_screen_grab_image \
    ${output} \
    ${SUB_ID}_dwifslpreproc_qc \
    ${input}
}

pipe_bias_correct()
{
  local input=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy.mif
  local output=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_N4.mif
  local mask=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_mask.mif

  run_if_missing ${mask} \
    dwi2mask "${input}" "${mask}" -nthreads "${THREADS}"

  run_if_missing "${output}" \
    dwibiascorrect ants "${input}" "${output}" \
    -mask "${mask}" \
    -nthreads "${THREADS}" \
    -scratch "${SCRATCH_DIR}" 
  check_exit $? "dwibiascorrect failed for ${input}"
}

pipe_upsample()
{
  local input=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_N4.mif
  local output=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_N4_${REGRID_SIZE_FORMATTED}.mif
  
  ### upsample if needed, otherwise symlink ###
  current_spacing=$(mrinfo "${input}" -spacing | cut -d' ' -f 1-3 | sed "s/ /,/g")
  if [[ "$current_spacing" != "${REGRID_SIZE}" ]]; then
    run_if_missing "${output}" \
      mrgrid "${input}" regrid -voxel ${REGRID_SIZE} "${output}" \
        -interp linear -force -nthreads "${THREADS}"
      check_exit $? "mrgrid failed for ${input}"
  else
    log "Regrid size ${REGRID_SIZE} already matches input spacing ${current_spacing}"
    run_if_missing "${output}" \
       ln -s "$(realpath "${input}")" "${output}"
  fi
}

pipe_tensor_fit()
{
  local input=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_N4_${REGRID_SIZE_FORMATTED}.mif 
  local output_dir="${OUT_BASE_DIR}/tensor_fit_${REGRID_SIZE_FORMATTED}"
  local mask="${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_mask.mif"
  local mask_regrid=${output_dir}/dwi_mask.nii.gz
  
  mkdir -p "${output_dir}"

  ### convert to nifti for dti fit ###
  local dwi_nii="${output_dir}/dwi.nii.gz"
  local bvec="${output_dir}/dwi.bvecs"
  local bval="${output_dir}/dwi.bvals"
  local mrtrix_grad="${output_dir}/dwi_grad_table"
  run_if_missing "${dwi_nii}" \
    mrconvert "${input}" "${dwi_nii}" \
      -export_grad_fsl "${bvec}" "${bval}" \
      -export_grad_mrtrix "${mrtrix_grad}" \
      -nthreads "${THREADS}" -info

  ### resize mask ###
  run_if_missing "${mask_regrid}" \
    mrgrid ${mask} \
      regrid -voxel ${REGRID_SIZE} ${mask_regrid} \
      -interp nearest -force -nthreads "${THREADS}"

  ### run DTI fitting ###
  run_if_missing "${output_dir}/dwi_FA.nii.gz" \
    dtifit \
      --data="${dwi_nii}" \
      --out="${output_dir}/dwi" \
      --mask="${mask_regrid}" \
      --bvecs="${bvec}" \
      --bvals="${bval}" \
      --save_tensor
  check_exit $? "dtifit failed for ${dwi_nii}"

  ### generate png of FA image and save to QC_DIR ###
  run_screen_grab_image ${output_dir}/dwi_FA.nii.gz \
    ${SUB_ID}_dwi_FA 
}

pipe_tbss()
{
  log "Running TBSS pipeline"

  local tensor_dir="${OUT_BASE_DIR}/tensor_fit_${REGRID_SIZE_FORMATTED}"
  local fa_image="${tensor_dir}/dwi_FA.nii.gz"
  local tensor_image="${tensor_dir}/dwi_tensor.nii.gz"
  local tbss_dir="${OUT_BASE_DIR}/tbss"
  
  mkdir -p "${tbss_dir}/FA"
  mkdir -p "${tbss_dir}/tensor"

  pushd "${tbss_dir}" > /dev/null

  ### Step 0: Prepare FA and tensor images for TBSS ###
  cp "${fa_image}" "FA/dwi_FA.nii.gz"
  cp "${tensor_image}" "tensor/dwi.nii.gz"

  ### Step 1: Preprocess FA (reorient and mask) ###
  log "Step 1: TBSS preprocessing"
  run_if_missing "FA/slicesdir/index.html" \
    tbss_1_preproc FA
  check_exit $? "tbss_1_preproc failed for ${fa_image}"

  ### Step 2: Register FA to FMRIB58 template ###
  log "Step 2: TBSS registration"
  run_if_missing "FA/dwi_FA_to_target_warp.nii.gz" \
    tbss_2_reg -T
  check_exit $? "tbss_2_reg failed for ${fa_image}"

  ### Step 3: Post-registration and skeleton ###
  log "Step 3: TBSS post-registration"
  run_if_missing "stats/all_FA.nii.gz" \
    tbss_3_postreg -S
  check_exit $? "tbss_3_postreg failed for ${fa_image}"

  ### Step 4: Threshold mean FA skeleton ###
  log "Step 4: TBSS skeleton thresholding"
  run_if_missing "stats/mean_FA_skeleton_mask_dst.nii.gz" \
    tbss_4_prestats 0.2
  check_exit $? "tbss_4_prestats failed for ${fa_image}"

  ### Step 5: Non-FA projection (tensor) ###
  log "Step 5: TBSS non-FA projection (tensor)"
  run_if_missing "FA/dwi_to_target_tensor.nii.gz" \
    tbss_non_FA tensor
  check_exit $? "tbss_non_FA failed for ${tensor_image}"

  ### QC snapshot ###
  run_screen_grab_image "${tbss_dir}/stats/mean_FA_skeleton.nii.gz" \
    ${SUB_ID}_tbss_skeleton

  popd > /dev/null
}

pipe_alps_extract()
{
 log "Running ALPS extraction"

  local tbss_tensor="${OUT_BASE_DIR}/tbss/FA/dwi_to_target_tensor.nii.gz"
  local alps_dir="${OUT_BASE_DIR}/alps"
  local tensor_dir="${alps_dir}/tensor"
  local comp_nifti="${tensor_dir}/tensor_components.nii.gz"
  local tensor_csv="${alps_dir}/alps_tensor_components.csv"
  local index_csv="${alps_dir}/alps_indices.csv"

  if [[ -f ${index_csv} ]]; then
    log "ALPS indices already exist, skipping extraction"
    return
  fi

  mkdir -p "${tensor_dir}"

  # Disassemble tensor components
  cp "${tbss_tensor}" "${tensor_dir}/tensor.nii.gz"
  ImageMath 4 "${comp_nifti}" TimeSeriesDisassemble "${tensor_dir}/tensor.nii.gz"

  # Rename components
  local i=0
  for comp in xx xy xz yy yz zz; do
    mv "${tensor_dir}/tensor_components100${i}.nii.gz" "${tensor_dir}/D${comp}.nii.gz"
    ((i++))
  done

  # Extract mean values for each ROI
  declare -A vals
  for hemi in L R; do
    vals[${hemi}_Dxproj]=$(fslstats "${tensor_dir}/Dxx.nii.gz" -k "${ALPS_ROI_DIR}/${hemi}_hemi_proj.nii.gz" -m)
    vals[${hemi}_Dxassoc]=$(fslstats "${tensor_dir}/Dxx.nii.gz" -k "${ALPS_ROI_DIR}/${hemi}_hemi_assoc.nii.gz" -m)
    vals[${hemi}_Dyproj]=$(fslstats "${tensor_dir}/Dyy.nii.gz" -k "${ALPS_ROI_DIR}/${hemi}_hemi_proj.nii.gz" -m)
    vals[${hemi}_Dzassoc]=$(fslstats "${tensor_dir}/Dzz.nii.gz" -k "${ALPS_ROI_DIR}/${hemi}_hemi_assoc.nii.gz" -m)
  done

  # Compute ALPS indices
  L_ALPS=$(echo "scale=4; (${vals[L_Dxproj]} + ${vals[L_Dxassoc]}) / (${vals[L_Dyproj]} + ${vals[L_Dzassoc]})" | bc)
  R_ALPS=$(echo "scale=4; (${vals[R_Dxproj]} + ${vals[R_Dxassoc]}) / (${vals[R_Dyproj]} + ${vals[R_Dzassoc]})" | bc)

  # Save tensor values CSV
  {
    echo "Subject,L_Dxproj,L_Dxassoc,L_Dyproj,L_Dzassoc,R_Dxproj,R_Dxassoc,R_Dyproj,R_Dzassoc"
    echo "${SUB_ID},${vals[L_Dxproj]},${vals[L_Dxassoc]},${vals[L_Dyproj]},${vals[L_Dzassoc]},"\
        "${vals[R_Dxproj]},${vals[R_Dxassoc]},${vals[R_Dyproj]},${vals[R_Dzassoc]}"
  } > "${tensor_csv}"

  # Save ALPS index CSV
  {
    echo "Subject,L_ALPS,R_ALPS"
    echo "${SUB_ID},${L_ALPS},${R_ALPS}"
  } > "${index_csv}"

  log "ALPS extraction complete: ${index_csv}"

}

#-------------------------------------------------------------------------------
### MAIN ###
#-------------------------------------------------------------------------------

main() 
{
  ### start pipeline ###
  parse_arguments "$@"
  setup_environment
  setup_directories  
  local start_time=$(date +%s)
  log "----------------------------------------"
  log "DTI pipeline STARTED for ${SUB_ID}: time $(date '+%F %T')"


  ### if no primary DTI is set, try to detect it ###
  if [[ -z "${PRIMARY_DTI_NII}" ]]; then
    log "Primary DTI not set: auto-detecting..."
    detect_dti_files
  fi

  ### prep for preprocessing ###
  verify_minimum_required_inputs

  if [[ ${DRY_RUN} -eq 1 ]]; then
    log "DRY_RUN is set to 1, skipping pipeline execution"
    return
  fi

  ### start pipeline steps ###
  pipe_mrconvert

  ### run initial dwi preprocessing ###
  pipe_dwi_preprocessing

  ## run initial rpe preprocessing (runs only when rpe data is detected) ###
  pipe_rpe_preprocessing

  ### run core dwi preprocessing ###
  pipe_dwipreproc 

  ### run bias correction ###
  pipe_bias_correct 

  ### run upsampling ###
  pipe_upsample

  ### run tensor fitting ###
  pipe_tensor_fit

  ### run tbss ###
  pipe_tbss

  ### run alps extraction ###
  pipe_alps_extract

  ### end pipeline ###
  local end_time=$(date +%s)
  local elapsed_seconds=$((end_time - start_time))
  local elapsed_minutes=$((elapsed_seconds / 60))
  local remaining_seconds=$((elapsed_seconds % 60))
  log "DTI pipeline COMPLETED for ${SUB_ID}: time $(date '+%F %T')"
  log "TOTAL PROCESSING TIME: ${elapsed_minutes} minutes ${remaining_seconds} seconds"
}

main "$@"

#-------------------------------------------------------------------------------
