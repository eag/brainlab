#!/bin/bash

### NOTES ###
# - this script performs standard DTI preprocessing steps most suitable for clinical data
# - EPI distortion correction is performed only if RPE data was acquired
# - expects input as the subject folder containing DTI data from DICOM conversion
#   - i.e. output from dcm2niix or equivalent (nii.gz, .bval, .bvec, and .json files)
# - processing steps determined primarily from image and json sidecar data found in
#   the input folder, together with a few user-defined variables and pipeline defaults


# TODO: currently assumes "b0_1.nii.gz" and "b0_2.nii.gz" suffix for dual b0 acquisitions

#===============================================================================
### DATASET CONFIG - GLOBAL VARIABLES ###
#===============================================================================

IN_DIR="${HOME}/tmp/test_datasets_dti/organized"
OUT_DIR="${HOME}/tmp/test_datasets_dti/derivatives/dti"

#IN_DIR="${SCRATCH}/organized"
#OUT_DIR="${SCRATCH}/derivatives/dti"

SUB_ID="BEM01_BYC_5002"
DWI_PFX="dwi"

#SUB_ID="sub-OND01BYC1006-sess-BL"
#DWI_PFX="DTI"

#SUB_ID="BEM01_CAM_5035_01_SE01_MR"
#DWI_PFX="DTI"

#SUB_ID="BCSA1F001"
#DWI_PFX="DTI"

#SUB_ID="empty"
#DWI_PFX="dd"

SCRATCH_DIR=${HOME}/tmp/
#SCRATCH_DIR=${SCRATCH}

QC_DIR="${OUT_DIR}/qc"
LOG_DIR="${OUT_DIR}/logs"
LOG_FN=${LOG_DIR}/${SUB_ID}_dwi_preprocess.log


ALPS_ROI_DIR=${HOME}/tmp/test_datasets_dti/alps-rois
THREADS=3
DRY_RUN=1


#-------------------------------------------------------------------------------
### SCRIPT CONFIG - GLOBAL VARIABLES ###
#-------------------------------------------------------------------------------

### input variables, autofilled later ###
PRIMARY_DTI_NII=""
PRIMARY_DTI_BVAL=""
PRIMARY_DTI_BVEC=""
PRIMARY_DTI_JSON=""
RPE_DWI_NII=""
RPE_BVAL=""
RPE_BVEC=""
RPE_JSON=""
RPE_B0_1=""
RPE_B0_2=""
RPE_B0_1_JSON=""
RPE_B0_2_JSON=""
RPE_MIF=""
SLICE_OR=""

### output variables ###
OUT_BASE_DIR="${OUT_DIR}/${SUB_ID}"

### pipeline defaults (for consistency across studies) ###
REGRID_SIZE="1,1,1" 

### convenience variables ###
REGRID_SIZE_FORMATTED=$(echo "${REGRID_SIZE}" | sed 's/,/_/g')

#-------------------------------------------------------------------------------
### HELPER FUNCTIONS ###
#-------------------------------------------------------------------------------

function log() 
{ 
  echo "[$(date '+%F %T')] $*" | tee -a "${LOG_FN}"; 
}

function setup_environment
{
  if [[ ! -d "${IN_DIR}/${SUB_ID}" ]]; then
    echo "ERROR: Input directory ${IN_DIR}/${SUB_ID} does not exist"
    exit 1
  fi

  if [[ -n ${CC_CLUSTER} ]]; then
    log "Running on CC cluster: ${CC_CLUSTER}"
    module load ants/2.5.0
    module load mrtrix/3.0.4
    module load fsl/6.0.7.7
  else
    export ANTSPATH=${HOME}/work/code/external-tools/ants/build/ANTS-build/Examples
    export PATH=${PATH}:${ANTSPATH}
  fi
}

run_if_missing() 
{
  local outfile="$1"
  shift
  if [[ ! -e "${outfile}" ]]; then
    log "Running: $*"
    "$@"
  else
    log "$(basename "${outfile}") already exists, skipping"
  fi
}

run_if_exists() 
{
  local outfile="$1"
  shift
  if [[ -e "${outfile}" ]]; then
    log "Running: $*"
    "$@"
  fi
}

#-------------------------------------------------------------------------------
### CONFIG FUNCTIONS ###
#-------------------------------------------------------------------------------

run_enforce_pe_convention() {
  ### for dual acquistions with two bvecs ###
  ### TODO: currently untested (no test data available) ###
  if [[ -f "${PRIMARY_DTI_JSON}" && -f "${RPE_JSON}" ]]; then
    local pe_primary
    local pe_rpe
    pe_primary=$(jq -r '.PhaseEncodingDirection' "${PRIMARY_DTI_JSON}")
    pe_rpe=$(jq -r '.PhaseEncodingDirection' "${RPE_JSON}")
    log "Primary PE: ${pe_primary}"
    log "RPE PE: ${pe_rpe}"

    if [[ "${pe_primary}" == "j-" && "${pe_rpe}" == "j" ]]; then
      log "Swapping primary and RPE DTI to enforce PE convention (prefer j over j-)"

      # Swap NII
      local tmp="${PRIMARY_DTI_NII}"
      PRIMARY_DTI_NII="${RPE_DWI_NII}"
      RPE_DWI_NII="${tmp}"

      # Swap BVAL
      tmp="${PRIMARY_DTI_BVAL}"
      PRIMARY_DTI_BVAL="${RPE_BVAL}"
      RPE_BVAL="${tmp}"

      # Swap BVEC
      tmp="${PRIMARY_DTI_BVEC}"
      PRIMARY_DTI_BVEC="${RPE_BVEC}"
      RPE_BVEC="${tmp}"

      # Swap JSON
      tmp="${PRIMARY_DTI_JSON}"
      PRIMARY_DTI_JSON="${RPE_JSON}"
      RPE_JSON="${tmp}"
    fi
  fi
}


detect_dti_files() {
  pushd "${IN_DIR}/${SUB_ID}" > /dev/null

  ### check for more than two DTI files ###
  dti_files=(${DWI_PFX}*.bvec)
  if (( ${#dti_files[@]} > 2 )); then
    log "ERROR: More than two DTI .nii.gz files found â€” unknown scenario"
    log "Please check input directory: ${IN_DIR}/${SUB_ID}/${DWI_PFX}*.nii.gz"
    exit 1
  fi

  ### first look for b0_1 and b0_2 ###
  RPE_B0_1=$(ls *b0_1*.nii.gz 2>/dev/null | head -n1)
  RPE_B0_2=$(ls *b0_2*.nii.gz 2>/dev/null | head -n1)
  if [[ -n "${RPE_B0_1}" && -n "${RPE_B0_2}" ]]; then
    RPE_B0_1_JSON="${RPE_B0_1%.nii.gz}.json"
    RPE_B0_2_JSON="${RPE_B0_2%.nii.gz}.json"

    if [[ -f "${RPE_B0_1}" && -f "${RPE_B0_1_JSON}" && -f "${RPE_B0_2}" && -f "${RPE_B0_2_JSON}" ]]; then
      log "Found two RPE b0 images, generating combined rpe.mif"
      RPE_MIF="${OUT_DIR}/${SUB_ID}/${DWI_PFX}_detected_combined_rpe.mif"
      run_if_missing "${RPE_MIF}" \
        run_create_combined_rpe "${RPE_B0_1}" "${RPE_B0_1_JSON}" "${RPE_B0_2}" "${RPE_B0_2_JSON}" "${RPE_MIF}"
    fi
  fi

  ### then look for primary DTI scan ###
  for nii in ${DWI_PFX}*.nii.gz; do
    base="${nii%.nii.gz}"
    bval="${base}.bval"
    bvec="${base}.bvec"
    json="${base}.json"

    if [[ -f "${nii}" && -f "${bval}" && -f "${bvec}" && -f "${json}" ]]; then
      PRIMARY_DTI_NII="${nii}"
      PRIMARY_DTI_BVAL="${bval}"
      PRIMARY_DTI_BVEC="${bvec}"
      PRIMARY_DTI_JSON="${json}"
      break  # stop searching after finding the first DTI scan
    fi
  done

  ### if no rpe.mif (b0_1/b0_2), look for second full DTI scan ###
  # TODO: untested
  if [[ -z "${RPE_MIF}" ]]; then
    for nii in ${DWI_PFX}*.nii.gz; do
      [[ "$nii" == "${PRIMARY_DTI_NII}" ]] && continue
      base="${nii%.nii.gz}"
      bval="${base}.bval"
      bvec="${base}.bvec"
      json="${base}.json"

      if [[ -f "${nii}" && -f "${bval}" && -f "${bvec}" && -f "${json}" ]]; then
        log "Found second DTI scan: WARNING untested scenario"
        run_enforce_pe_convention
        RPE_MIF="${OUT_DIR}/${SUB_ID}/${DWI_PFX}_detected_rpe.mif"
        run_create_rpe "${nii}" "${json}" "${RPE_MIF}"
        break
      fi
    done
  fi

  ### if still no rpe.mif, look for standalone B0 volume with json only ###
  if [[ -z "${RPE_MIF}" ]]; then
    for nii in *.nii.gz; do
      [[ "$nii" == "${PRIMARY_DTI_NII}" ]] && continue
      base="${nii%.nii.gz}"
      bval="${base}.bval"
      bvec="${base}.bvec"
      json="${base}.json"

      if [[ -f "${json}" && ! -f "${bval}" && ! -f "${bvec}" ]]; then
        log "Found single-volume B0"
        RPE_DWI_NII="${nii}"
        RPE_MIF="${OUT_DIR}/${SUB_ID}/${DWI_PFX}_detected_rpe.mif"
        run_create_rpe "${nii}" "${json}" "${RPE_MIF}"
        break
      fi
    done
  fi

  ### finalize full paths for non-empty variables ###
  for var in PRIMARY_DTI_NII PRIMARY_DTI_BVAL PRIMARY_DTI_BVEC PRIMARY_DTI_JSON \
             RPE_B0_1 RPE_B0_2 RPE_B0_1_JSON RPE_B0_2_JSON; do
    val="${!var}"
    [[ -n "$val" ]] && eval "$var=\"${IN_DIR}/${SUB_ID}/$val\""
  done

  ### log whether RPE data was found ###
  [[ -n "${RPE_MIF}" ]] && log "RPE data: ${RPE_MIF}" || log "No RPE data found"

  popd > /dev/null
}

verify_minimum_required_inputs()
{
  log "PRIMARY_DTI_NII : $(basename "${PRIMARY_DTI_NII:-}")"
  log "PRIMARY_DTI_BVAL: $(basename "${PRIMARY_DTI_BVAL:-}")"
  log "PRIMARY_DTI_BVEC: $(basename "${PRIMARY_DTI_BVEC:-}")"
  log "PRIMARY_DTI_JSON: $(basename "${PRIMARY_DTI_JSON:-}")"
  log "RPE_DWI_NII    : $(basename "${RPE_DWI_NII:-}")"
  log "RPE_BVAL      : $(basename "${RPE_BVAL:-}")"
  log "RPE_BVEC      : $(basename "${RPE_BVEC:-}")"
  log "RPE_JSON      : $(basename "${RPE_JSON:-}")"
  log "RPE_B0_1      : $(basename "${RPE_B0_1:-}")"
  log "RPE_B0_2      : $(basename "${RPE_B0_2:-}")"
  log "RPE_B0_1_JSON  : $(basename "${RPE_B0_1_JSON:-}")"
  log "RPE_B0_2_JSON  : $(basename "${RPE_B0_2_JSON:-}")"
  log "RPE_MIF       : $(basename "${RPE_MIF:-}")"

  if [[ -z "${PRIMARY_DTI_NII}" || -z "${PRIMARY_DTI_BVAL}" || \
        -z "${PRIMARY_DTI_BVEC}" || -z "${PRIMARY_DTI_JSON}" ]]; then
    log "ERROR: required input [nii.gz + json + bvec + bval] not found"
    log "Please check input directory: ${IN_DIR}/${SUB_ID}/${DWI_PFX}*"
    exit 1
  fi
}

detect_slice_orientation_from_json() 
{
  local json_file="$1"

  if [[ ! -f "${json_file}" ]]; then
    return
  fi

  local vals
  vals=($(jq -r '.ImageOrientationPatientDICOM[]?' "${json_file}"))

  if [[ "${#vals[@]}" -ne 6 ]]; then
    return
  fi

  local Xx="${vals[0]}" Xy="${vals[1]}" Xz="${vals[2]}"
  local Yx="${vals[3]}" Yy="${vals[4]}" Yz="${vals[5]}"

  local Sx Sy Sz
  Sx=$(echo "${Xy} * ${Yz} - ${Xz} * ${Yy}" | bc -l)
  Sy=$(echo "${Xz} * ${Yx} - ${Xx} * ${Yz}" | bc -l)
  Sz=$(echo "${Xx} * ${Yy} - ${Xy} * ${Yx}" | bc -l)

  local abs_Sx abs_Sy abs_Sz
  abs_Sx=$(echo "${Sx#-}" | bc -l)
  abs_Sy=$(echo "${Sy#-}" | bc -l)
  abs_Sz=$(echo "${Sz#-}" | bc -l)

  if (( $(echo "${abs_Sz} >= ${abs_Sx} && ${abs_Sz} >= ${abs_Sy}" | bc -l) )); then
    SLICE_OR="0,1"  # axial
  elif (( $(echo "${abs_Sy} >= ${abs_Sx} && ${abs_Sy} >= ${abs_Sz}" | bc -l) )); then
    SLICE_OR="0,2"  # coronal
  else
    SLICE_OR="1,2" # sagittal
  fi
  log "Slice orientation detected: ${SLICE_OR}"

  if [[ "${SLICE_OR}" != "0,1" ]]; then
    log "WARNING: Slice orientation is not axial (0,1). Detected: ${SLICE_OR}"
  fi
}

#-------------------------------------------------------------------------------
### PREPROCESSING FUNCTIONS ###
#-------------------------------------------------------------------------------
# - "pipe_" : pipeline functions that operate on staged filenames
# - "run_"  : utility functions used by pipe functions with filenames as arguments


run_create_rpe()
{
  local nii="$1"
  local json="$2"
  local out_mif="$3"

  if [[ ${DRY_RUN} -eq 1 ]]; then
    log "Skipping RPE creation for test run"
    return
  fi

  run_if_missing "${out_mif}" \
    mrconvert "${nii}" "${out_mif}" -json_import "${json}" -quiet -force
}

run_create_combined_rpe() {
  local b0_1="$1"
  local b0_1_json="$2"
  local b0_2="$3"
  local b0_2_json="$4"
  local out_mif="$5"

  if [[ ${DRY_RUN} -eq 1 ]]; then
    log "Skipping RPE creation for test run"
    return
  fi

  if [[ -f "${out_mif}" ]]; then
    log "RPE already exists, skipping creation"
    exit 0
  fi

  local tmp1=$(mktemp "$(pwd)/tmp_rpe_$(date +%Y%m%d)_XXXXXX.mif")
  local tmp2=$(mktemp "$(pwd)/tmp_rpe_$(date +%Y%m%d)_XXXXXX.mif")

  mrconvert "${b0_1}" "${tmp1}" -json_import "${b0_1_json}" -quiet -force
  mrconvert "${b0_2}" "${tmp2}" -json_import "${b0_2_json}" -quiet -force
  mrcat "${tmp1}" "${tmp2}" -axis 3 "${out_mif}" -quiet -force

  rm -f "${tmp1}" "${tmp2}"
}


run_dwidenoise() 
{
  local input="$1"
  local output="$2"
  run_if_missing "${output}" \
    dwidenoise "${input}" "${output}" -nthreads "${THREADS}"
}

run_degibbs() 
{
  local input="$1"
  local output="$2"
  detect_slice_orientation_from_json "${PRIMARY_DTI_JSON}"
  
  if [[ -z "${SLICE_OR}" ]]; then
    log "Slice orientation not detected, using default (0,1)"
    SLICE_OR="0,1"
  fi
  
  run_if_missing "${output}" \
    mrdegibbs "${input}" "${output}" -nthreads "${THREADS}" -axes "${SLICE_OR}"
}
run_screen_grab_image()
{
  local input="$1"
  local output_png_pfx="$2"
  local overlay="$3"
  local output_dir=$(dirname "${input}")

  local gui_runner=""
  if [[ "${CC_CLUSTER}" == "cedar" ]]; then
    gui_runner="run-xvfb"
  elif [[ "${CC_CLUSTER}" == "narval" ]]; then
    gui_runner=xvfb-run
  fi

  if [[ -n "${overlay}" && ! -f "${overlay}" ]]; then
    log "ERROR: Overlay file ${overlay} does not exist"
    exit 1
  fi

  pushd "${output_dir}" > /dev/null

  local cmd="${gui_runner} mrview ${input}"
  if [[ -n "${overlay}" ]]; then
    cmd+=" -overlay.load ${overlay} -overlay.opacity 0.5 -overlay.colourmap 1"
  fi
  cmd+=" -mode 2 -capture.prefix ${output_png_pfx}_ -capture.folder ${QC_DIR} -capture.grab -exit"

  run_if_missing "${QC_DIR}/${output_png_pfx}_0000.png" \
    run_if_exists "${input}" \
      bash -c "${cmd}"

  popd > /dev/null
}


run_qc_residual() 
{
  local original=$1
  local processed=$2
  local output=$3
  local png_pfx=$4

  ### generate residual image ###
  run_if_missing ${output} \
    mrcalc "${original}" "${processed}" -subtract ${output} -force -nthreads "${THREADS}"

  ### generate png of residual image ###
  run_screen_grab_image "${output}" "${png_pfx}"
}

pipe_mrconvert()
{
  run_if_missing "${OUT_BASE_DIR}/dwi.mif" \
    mrconvert ${PRIMARY_DTI_NII} "${OUT_BASE_DIR}/dwi.mif" \
      -fslgrad ${PRIMARY_DTI_BVEC} ${PRIMARY_DTI_BVAL} \
      -json_import ${PRIMARY_DTI_JSON} \
      -nthreads "${THREADS}"
}

run_quantatitive_eddy_qc()
{
  local eddy_dir="${OUT_BASE_DIR}/eddyqc_dir"
  local input_dwi="${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy.mif"
  local output_csv="${QC_DIR}/${SUB_ID}_eddy_qc_quantitative.csv"

  local file_movement_rms="${eddy_dir}/eddy_movement_rms"
  local file_restricted_movement_rms="${eddy_dir}/eddy_restricted_movement_rms"
  local file_outlier_map="${eddy_dir}/eddy_outlier_map"

  ### extract mean absolute motion (second column) ###
  mean_abs_motion=$(awk '{sum+=$2} END {if (NR>0) print sum/NR; else print "NA"}' "$file_movement_rms")

  ### extract mean relative motion (second column) ###
  mean_rel_motion=$(awk '{sum+=$2} END {if (NR>0) print sum/NR; else print "NA"}' "$file_restricted_movement_rms")

  ### calculate percent outlier slices ###
  n_outlier_slices=$(wc -l < "$file_outlier_map")

  n_volumes=$(mrinfo "$input_dwi" -size | awk '{print $4}')
  n_slices=$(mrinfo "$input_dwi" -size | awk '{print $3}')
  total_slices=$(( n_volumes * n_slices ))

  if [[ $total_slices -gt 0 ]]; then
    percent_outliers=$(awk -v outliers="$n_outlier_slices" -v total="$total_slices" 'BEGIN {print 100*outliers/total}')
  else
    percent_outliers="NA"
  fi

  ### write output CSV ###
  {
    echo "MeanAbsMotion,MeanRelMotion,PercentOutliers"
    echo "${mean_abs_motion},${mean_rel_motion},${percent_outliers}"
  } > "$output_csv"
}

pipe_dwi_preprocessing()
{
  log "preprocessing DWI"

  run_dwidenoise ${OUT_BASE_DIR}/dwi.mif \
    ${OUT_BASE_DIR}/dwi_denoised.mif
  
  run_degibbs ${OUT_BASE_DIR}/dwi_denoised.mif \
    ${OUT_BASE_DIR}/dwi_denoised_degibbs.mif

  run_qc_residual ${OUT_BASE_DIR}/dwi.mif \
    ${OUT_BASE_DIR}/dwi_denoised.mif \
    ${OUT_BASE_DIR}/dwi_denoised_residuals.mif \
    ${SUB_ID}_dwi_denoised_residuals
    
  run_qc_residual ${OUT_BASE_DIR}/dwi_denoised.mif \
    ${OUT_BASE_DIR}/dwi_denoised_degibbs.mif \
    ${OUT_BASE_DIR}/dwi_denoised_degibbs_residuals.mif \
    ${SUB_ID}_dwi_denoised_degibbs_residuals
}

pipe_rpe_preprocessing()
{
  ### preprocess rpe_b0, if available (scenario 2) ###
  if [[ -f "${RPE_MIF}" && -z "${RPE_BVEC}" ]]; then
    log "preprocessing RPE B0"
    log "denoising set to null operation"
    run_degibbs ${RPE_MIF} \
      "${OUT_BASE_DIR}/rpe_denoised_degibbs.mif"

    run_qc_residual ${RPE_MIF} \
      ${OUT_BASE_DIR}/rpe_denoised_degibbs.mif \
      ${OUT_BASE_DIR}/rpe_denoised_degibbs_residuals.mif \
      ${SUB_ID}_rpe_denoised_degibbs_residuals
  fi

  ### preprocess rpe_dwi, if available (scenario 3) ###
  #if [[ -n "${RPE_DWI_NII}" ]]; then
  #  log "preprocessing RPE DWI"
  #
  #  run_dwidenoise "${OUT_BASE_DIR}/rpe_dwi.mif" \
  #    "${OUT_BASE_DIR}/rpe_dwi_denoised.mif"
  #  
  #  run_degibbs "${OUT_BASE_DIR}/rpe_dwi_denoised.mif" \
  #    "${OUT_BASE_DIR}/rpe_dwi_denoised_degibbs.mif"

  ### qc residuals ###
  # run_qc_residual ${OUT_BASE_DIR}/rpe_b0_dwi.mif \
  #   ${OUT_BASE_DIR}/rpe_denoised_degibbs.mif \
  #   ${OUT_BASE_DIR}/rpe_denoised_degibbs_residuals.mif \
  #   ${SUB_ID}_rpe_denoised_degibbs_residuals

  #fi

  
}

pipe_dwipreproc()
{
  local input=${OUT_BASE_DIR}/dwi_denoised_degibbs.mif
  local output=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy.mif
  local scratch_dir=${SCRATCH_DIR}/tmp_${SUB_ID}_$(date +%Y%m%d_%H%M%S)
  
  if [[ -n "${RPE_DWI_NII}" ]]; then
    # scenario 3
    echo "TODO: implement scenario 3 -- scenario 2 should work"
  elif [[ -n "${RPE_B0_NII}" ]]; then
    # scenario 2
    local rpe=${OUT_BASE_DIR}/rpe_denoised_degibbs.mif
    log "Running dwifslpreproc for DWI RPE B0 acquistion (scenario 2)"
    run_if_missing "${output}" \
      dwifslpreproc ${input} ${output} \
        -rpe_header \
        -se_epi ${rpe} -align_seepi \
        -eddy_options "--slm=linear --repol --data_is_shelled --verbose --estimate_move_by_susceptibility" \
        -eddyqc_all ${OUT_BASE_DIR}/eddyqc_dir \
        -info \
        -scratch ${scratch_dir} \
        -nthreads "${THREADS}" \
        -force
  else
    # scenario 1
    log "Running dwifslpreproc for DWI no RPE acquisition (scenario 1)"
    run_if_missing ${output} \
      dwifslpreproc ${input} ${output} \
        -rpe_header \
        -eddy_options "--slm=linear --repol --data_is_shelled --verbose" \
        -eddyqc_all ${OUT_BASE_DIR}/eddyqc_dir \
        -info \
        -scratch ${scratch_dir} \
        -nthreads "${THREADS}" \
        -force
  fi

  ### QC dwifslpreproc ###
  cp ${OUT_BASE_DIR}/eddyqc_dir/quad/qc.pdf ${QC_DIR}/${SUB_ID}_quad_eddy_qc.pdf
  run_quantatitive_eddy_qc
  run_screen_grab_image \
    ${output} \
    ${SUB_ID}_dwifslpreproc_qc \
    ${input}
}

pipe_bias_correct()
{
  local input=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy.mif
  local output=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_N4.mif
  local mask=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_mask.mif

  run_if_missing ${mask} \
    dwi2mask "${input}" "${mask}" -nthreads "${THREADS}"

  run_if_missing "${output}" \
    dwibiascorrect ants "${input}" "${output}" \
    -mask "${mask}" \
    -nthreads "${THREADS}" \
    -scratch "${SCRATCH_DIR}" 
}

pipe_upsample()
{
  local input=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_N4.mif
  local output=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_N4_${REGRID_SIZE_FORMATTED}.mif
  
  ### upsample if needed, otherwise symlink ###
  current_spacing=$(mrinfo "${input}" -spacing | cut -d' ' -f 1-3 | sed "s/ /,/g")
  if [[ "$current_spacing" != "${REGRID_SIZE}" ]]; then
    run_if_missing "${output}" \
      mrgrid "${input}" regrid -voxel ${REGRID_SIZE} "${output}" \
      -interp linear -force -nthreads "${THREADS}"
  else
    log "Regrid size ${REGRID_SIZE} already matches input spacing ${current_spacing}"
    run_if_missing "${output}" \
       ln -s "$(realpath "${input}")" "${output}"
  fi
}

pipe_tensor_fit()
{
  local input=${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_N4_${REGRID_SIZE_FORMATTED}.mif 
  local output_dir="${OUT_BASE_DIR}/tensor_fit_${REGRID_SIZE_FORMATTED}"
  local mask="${OUT_BASE_DIR}/dwi_denoised_degibbs_eddy_mask.mif"
  local mask_regrid=${output_dir}/dwi_mask.nii.gz
  
  mkdir -p "${output_dir}"

  ### convert to nifti for dti fit ###
  local dwi_nii="${output_dir}/dwi.nii.gz"
  local bvec="${output_dir}/dwi.bvecs"
  local bval="${output_dir}/dwi.bvals"
  local mrtrix_grad="${output_dir}/dwi_grad_table"
  run_if_missing "${dwi_nii}" \
    mrconvert "${input}" "${dwi_nii}" \
      -export_grad_fsl "${bvec}" "${bval}" \
      -export_grad_mrtrix "${mrtrix_grad}" \
      -nthreads "${THREADS}" -info

  ### resize mask ###
  run_if_missing "${mask_regrid}" \
    mrgrid ${mask} \
      regrid -voxel ${REGRID_SIZE} ${mask_regrid} \
      -interp nearest -force -nthreads "${THREADS}"

  ### run DTI fitting ###
  run_if_missing "${output_dir}/dwi_FA.nii.gz" \
    dtifit \
      --data="${dwi_nii}" \
      --out="${output_dir}/dwi" \
      --mask="${mask_regrid}" \
      --bvecs="${bvec}" \
      --bvals="${bval}" \
      --save_tensor

  ### generate png of FA image and save to QC_DIR ###
  run_screen_grab_image ${output_dir}/dwi_FA.nii.gz \
    ${SUB_ID}_dwi_FA 
}

pipe_tbss()
{
  log "Running TBSS pipeline"

  local tensor_dir="${OUT_BASE_DIR}/tensor_fit_${REGRID_SIZE_FORMATTED}"
  local fa_image="${tensor_dir}/dwi_FA.nii.gz"
  local tensor_image="${tensor_dir}/dwi_tensor.nii.gz"
  local tbss_dir="${OUT_BASE_DIR}/tbss"
  
  mkdir -p "${tbss_dir}/FA"
  mkdir -p "${tbss_dir}/tensor"

  pushd "${tbss_dir}" > /dev/null

  ### Step 0: Prepare FA and tensor images for TBSS ###
  cp "${fa_image}" "FA/dwi_FA.nii.gz"
  cp "${tensor_image}" "tensor/dwi.nii.gz"

  ### Step 1: Preprocess FA (reorient and mask) ###
  log "Step 1: TBSS preprocessing"
  run_if_missing "FA/slicesdir/index.html" \
    tbss_1_preproc FA

  ### Step 2: Register FA to FMRIB58 template ###
  log "Step 2: TBSS registration"
  run_if_missing "FA/${fa_base}_to_target_warp.nii.gz" \
    tbss_2_reg -T

  ### Step 3: Post-registration and skeleton ###
  log "Step 3: TBSS post-registration"
  run_if_missing "stats/all_FA.nii.gz" \
    tbss_3_postreg -S

  ### Step 4: Threshold mean FA skeleton ###
  log "Step 4: TBSS skeleton thresholding"
  run_if_missing "stats/mean_FA_skeleton_mask_dst.nii.gz" \
    tbss_4_prestats 0.2

  ### Step 5: Non-FA projection (tensor) ###
  log "Step 5: TBSS non-FA projection (tensor)"
  run_if_missing "FA/dwi_to_target_tensor.nii.gz"
    tbss_non_FA tensor

  ### QC snapshot ###
  run_screen_grab_image "stats/mean_FA_skeleton.nii.gz" \
    "${SUB_ID}_tbss_skeleton" "stats/mean_FA.nii.gz"

  popd > /dev/null
}


pipe_alps_extract()
{
  log "Running ALPS extraction"

  local tensor_projected="${OUT_BASE_DIR}/tbss/FA/dwi_to_target_tensor.nii.gz"
  local alps_dir="${OUT_BASE_DIR}/alps"
  local tensor_dir="${alps_dir}/tensor"
  local output_csv="${alps_dir}/alps_metrics.csv"

  mkdir -p "${tensor_dir}"

  # Disassemble tensor components
  cp "${tensor_projected}" "${tensor_dir}/FA.nii.gz"
  ImageMath 4 "${tensor_dir}/tensor_comp.nii.gz" TimeSeriesDisassemble "${tensor_dir}/FA.nii.gz"

  local i=0
  for comp in xx xy xz yy yz zz; do
    mv "${tensor_dir}/tensor_comp100${i}.nii.gz" "${tensor_dir}/tensor_D${comp}.nii.gz"
    i=$((i+1))
  done

  # Extract mean values from each ROI
  L_Dxproj=$(fslstats "${tensor_dir}/tensor_Dxx.nii.gz" -k "${ALPS_ROI_DIR}/L_hemi_proj.nii.gz" -m)
  L_Dxassoc=$(fslstats "${tensor_dir}/tensor_Dxx.nii.gz" -k "${ALPS_ROI_DIR}/L_hemi_assoc.nii.gz" -m)
  L_Dyproj=$(fslstats "${tensor_dir}/tensor_Dyy.nii.gz" -k "${ALPS_ROI_DIR}/L_hemi_proj.nii.gz" -m)
  L_Dzassoc=$(fslstats "${tensor_dir}/tensor_Dzz.nii.gz" -k "${ALPS_ROI_DIR}/L_hemi_assoc.nii.gz" -m)
  R_Dxproj=$(fslstats "${tensor_dir}/tensor_Dxx.nii.gz" -k "${ALPS_ROI_DIR}/R_hemi_proj.nii.gz" -m)
  R_Dxassoc=$(fslstats "${tensor_dir}/tensor_Dxx.nii.gz" -k "${ALPS_ROI_DIR}/R_hemi_assoc.nii.gz" -m)
  R_Dyproj=$(fslstats "${tensor_dir}/tensor_Dyy.nii.gz" -k "${ALPS_ROI_DIR}/R_hemi_proj.nii.gz" -m)
  R_Dzassoc=$(fslstats "${tensor_dir}/tensor_Dzz.nii.gz" -k "${ALPS_ROI_DIR}/R_hemi_assoc.nii.gz" -m)

  # Compute ALPS indices
  L_ALPS=$(echo "scale=4; (${L_Dxproj} + ${L_Dxassoc}) / (${L_Dyproj} + ${L_Dzassoc})" | bc)
  R_ALPS=$(echo "scale=4; (${R_Dxproj} + ${R_Dxassoc}) / (${R_Dyproj} + ${R_Dzassoc})" | bc)

  # Write to CSV all above values
  {
    echo "Subject,L_Dxproj,L_Dxassoc,L_Dyproj,L_Dzassoc,R_Dxproj,R_Dxassoc,R_Dyproj,R_Dzassoc"
    echo "${SUB_ID},${L_Dxproj},${L_Dxassoc},${L_Dyproj},${L_Dzassoc},${R_Dxproj},${R_Dxassoc},${R_Dyproj},${R_Dzassoc}"
  } > "${alps_dir}/alps_tensor_components.csv"  
  
  # Write to CSV ALPS indices
  {
    echo "Subject,L_ALPS,R_ALPS"
    echo "${SUB_ID},${L_ALPS},${R_ALPS}"
  } > "${output_csv}"

  log "ALPS extraction complete: ${output_csv}"
}


pipe_cleanup()
{
  log "Cleaning up intermediate files"

  rm -f "${OUT_BASE_DIR}"/dwi*.mif
  rm -f "${OUT_BASE_DIR}"/eddyqc_dir/eddy_outlier_*.nii.gz
  rm -f "${OUT_BASE_DIR}"/eddyqc_dir/eddy_mask.nii
  rm -f "${OUT_BASE_DIR}"/dwi_denoised_*_residuals.mif

  # optionally remove tbss logs and slicesdir
  rm -rf "${OUT_BASE_DIR}/tbss/FA/tbss_logs"
  tar czf "${OUT_BASE_DIR}/tbss_slicesdir.tar.gz" -C "${OUT_BASE_DIR}/tbss/FA" slicesdir
  rm -rf "${OUT_BASE_DIR}/tbss/FA/slicesdir"

  # optionally remove disassembled FA
  rm -f "${OUT_BASE_DIR}/alps/tensor/FA.nii.gz"
  rm -f "${OUT_BASE_DIR}/alps/tensor/tensor_comp*.nii.gz"
}

#-------------------------------------------------------------------------------
### MAIN ###
#-------------------------------------------------------------------------------

main() 
{
  local start_time=$(date +%s)
  log "DTI pipeline STARTED for ${SUB_ID}: time $(date '+%F %T')"
  
  setup_environment
  
  ### if no primary DTI is set, try to detect it ###
  if [[ -z "${PRIMARY_DTI_NII}" ]]; then
    log "Primary DTI not set: auto-detecting..."
    detect_dti_files
  fi

  ### prep for preprocessing ###
  verify_minimum_required_inputs

  if [[ ${DRY_RUN} -eq 1 ]]; then
    log "DRY_RUN is set to 1, skipping pipeline execution"
    return
  fi

  ### start pipeline steps ###
  pipe_mrconvert

  ### run initial dwi preprocessing ###
  pipe_dwi_preprocessing

  ## run initial rpe preprocessing (runs only when rpe data is detected) ###
  pipe_rpe_preprocessing

  ### run core dwi preprocessing ###
  pipe_dwipreproc 

  ### run bias correction ###
  pipe_bias_correct 

  ### run upsampling ###
  pipe_upsample

  ### run tensor fitting ###
  pipe_tensor_fit

  ### run tbss ###
  pipe_tbss

  ### run alps extraction ###
  pipe_alps_extract

  ### run cleanup ###
  #pipe_cleanup

  local end_time=$(date +%s)
  local elapsed_seconds=$((end_time - start_time))
  local elapsed_minutes=$((elapsed_seconds / 60))
  local remaining_seconds=$((elapsed_seconds % 60))

  log "DTI pipeline COMPLETED for ${SUB_ID}: time $(date '+%F %T')"
  log "TOTAL PROCESSING TIME: ${elapsed_minutes} minutes ${remaining_seconds} seconds"

}


main "$@"

#-------------------------------------------------------------------------------
