#!/bin/bash
# ------------------------------------------------------------------------------
# @file sbt_dwi_preprocess
#
# This script performs standard DTI preprocessing steps suitable for clinical data.
# EPI distortion correction is performed only if RPE data was acquired.
# Expects input as the subject folder containing DTI data from DICOM conversion,
# i.e., output from dcm2niix or equivalent (nii.gz, .bval, .bvec, and .json files).
#
# The input folder must contain either:
#   1. A single DTI acquisition, or
#   2. A DTI acquisition uniquely identified by the prefix <dwi_pfx*> 
#      (i.e., ls <input_folder>/<dwi_pfx>* returns only that acquisition).
#
# The output folder for the SUB_ID will contain the following:
#   - dwi_denoised_degibbs_eddy_N4.mif: preprocessed DWI data 
#     denoised + Gibbs artifact corrected + <EPI distortion corrected> + bias field corrected DTI
#   - dwi_denoised_degibbs_eddy_N4_<regrid_size>.mif: upsampled preprocessed DWI data
#   - <regrid_size>_tensor_fit: directory containing tensor fit results
#   - <regrid_size>_tbss: directory containing TBSS results
#   - <regrid_size>_alps: directory containing ALPS results
#   - ../qc: directory containing QC images
#   - ../logs: directory containing log files
#
# Note:
# - Does not explicitly verify that the input data is a DTI scan. If not, the script
#   will simply attempt to process as DTI data and fail.
# - Processing steps are determined primarily from image and JSON sidecar data found
#   in the input folder, with the specific processing commands written to the log file.
#
# @author Erin Gibson
# @date 2025-05-02
#
# TODO:
# - Currently assumes "b0_1.nii.gz" and "b0_2.nii.gz" suffix for dual b0 acquisitions.
# - Dual full DTI acquisitions not yet tested/implemented.
# ------------------------------------------------------------------------------

#===============================================================================
### PARSE ARGUMENTS AND SET GLOBAL VARIABLES ###
#===============================================================================

parse_arguments() 
{
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --in_dir|--out_dir|--scratch_dir_root|--alps_roi_dir|--threads|--dry_run|--sub_id|--dwi_pfx|--regrid_size|--cleanup)
        flag=$(echo "${1#--}" | tr '[:lower:]' '[:upper:]')  # e.g., in_dir -> IN_DIR
        value="$2"
        if [[ -z "$value" || "$value" == --* ]]; then
          echo "ERROR: Missing value for $1" >&2
          exit 1
        fi
        declare -g "${flag}=$value"  # sets global variable like IN_DIR="/path"
        shift 2
        ;;
      -h|--help)
        echo "Usage: $0 --in_dir DIR --out_dir DIR --scratch_dir_root DIR --alps_roi_dir DIR"
        echo "          --threads N --dry_run 0|1 --sub_id ID --dwi_pfx PFX --regrid_size X,Y,Z --cleanup 0|1"
        echo -e "Notes\n  PFX=\"*\" for folders containing a single DTI acquisition"
        exit 0
        ;;
      *)
        echo "ERROR: Unknown argument: $1" >&2
        exit 1
        ;;
    esac
  done

  ### confirm all required global variables are set ###
  required_vars=(
    IN_DIR OUT_DIR SCRATCH_DIR_ROOT ALPS_ROI_DIR THREADS DRY_RUN SUB_ID DWI_PFX REGRID_SIZE CLEANUP
  )
  for var in "${required_vars[@]}"; do
    if [[ -z "${!var}" ]]; then
      echo "ERROR: Missing required argument --$(echo "$var" | tr '[:upper:]' '[:lower:]')" >&2
      exit 1
    fi
  done

  ### set derived global variables ###
  OUT_DIR_ROOT="${OUT_DIR}/${SUB_ID}"
  QC_DIR="${OUT_DIR}/qc"
  LOG_DIR="${OUT_DIR}/logs"
  LOG_FN=${LOG_DIR}/${SUB_ID}_dwi_preprocess.log
  REGRID_SIZE_FORMATTED=$(echo "${REGRID_SIZE}" | sed 's/,/_/g')
}

#-------------------------------------------------------------------------------
### GLOBAL VARIABLES (auto-filled later) ###
#-------------------------------------------------------------------------------

PRIMARY_DWI_NII=""
PRIMARY_DWI_BVAL=""
PRIMARY_DWI_BVEC=""
PRIMARY_DWI_JSON=""
RPE_DWI_NII=""
RPE_BVAL=""
RPE_BVEC=""
RPE_JSON=""
RPE_B0_1=""
RPE_B0_2=""
RPE_B0_1_JSON=""
RPE_B0_2_JSON=""
RPE_MIF=""
SLICE_OR=""

#-------------------------------------------------------------------------------
### HELPER FUNCTIONS ###
#-------------------------------------------------------------------------------

log() 
{ 
  echo "[$(date '+%F %T')] $*" | tee -a "${LOG_FN}"; 
}

setup_environment()
{
  if [[ -n ${CC_CLUSTER} ]]; then
    module load ants/2.5.0
    module load mrtrix/3.0.4
    module load fsl/6.0.7.7
  else
    ### TODO: update for local environment ###
    export ANTSPATH=${HOME}/work/code/external-tools/ants/build/ANTS-build/Examples
    export PATH=${PATH}:${ANTSPATH}
  fi
}

setup_directories()
{
  ### check existence of required input directories ###
  declare -A REQUIRED_DIRS=(
    ["Input directory"]="${IN_DIR}/${SUB_ID}"
    ["Scratch directory root"]="${SCRATCH_DIR_ROOT}"
    ["ALPS ROI directory"]="${ALPS_ROI_DIR}"
  )

  for label in "${!REQUIRED_DIRS[@]}"; do
    dir="${REQUIRED_DIRS[$label]}"
    if [[ ! -d "${dir}" ]]; then
      echo "ERROR: ${label} does not exist: ${dir}" >&2
      exit 1
    fi
  done

  ### create output directories  ###
  for dir in "${OUT_DIR_ROOT}" "${QC_DIR}" "${LOG_DIR}"; do
    mkdir -p "${dir}" || 
    {
      echo "ERROR: Failed to create output directory: ${dir}" >&2
      exit 1
    }
  done

  ### check writability of output directories ###
  declare -A WRITABLE_DIRS=(
    ["Scratch directory"]="${SCRATCH_DIR_ROOT}"
    ["Output base directory"]="${OUT_DIR_ROOT}"
  )

  for label in "${!WRITABLE_DIRS[@]}"; do
    dir="${WRITABLE_DIRS[$label]}"
    if [[ ! -w "${dir}" ]]; then
      echo "ERROR: ${label} not writable: ${dir}" >&2
      exit 1
    fi
  done

  ### guard against unsafe SCRATCH_DIR_ROOT directory (bad symlinks etc) ###
  SCRATCH_PARENT=$(realpath "${SCRATCH_DIR_ROOT}")
  if [[ -z "${SCRATCH_PARENT}" || "${SCRATCH_PARENT}" == "/" ]]; then
    echo "ERROR: Invalid or unsafe scratch parent: ${SCRATCH_DIR_ROOT}" >&2
    exit 1
  fi

  ### create TMP_SCRATCH_DIR ###
  TMP_SCRATCH_DIR="${SCRATCH_PARENT}/tmp_${SUB_ID}_$(date +%Y%m%d_%H%M%S)"
  if [[ ${DRY_RUN} -eq 0 ]]; then
    log "Creating scratch directory: ${TMP_SCRATCH_DIR}"
    mkdir ${TMP_SCRATCH_DIR} || 
    {
      echo "ERROR: Failed to create scratch directory: ${TMP_SCRATCH_DIR}" >&2
      exit 1
    }
  else
    log "Skipping scratch directory creation for test run: ${TMP_SCRATCH_DIR}"
  fi
}

run_if_missing() 
{
  local outfile="$1"
  shift
  if [[ ! -e "${outfile}" ]]; then
    log "Running: $*"
    "$@"
  else
    log "$(basename "${outfile}") already exists, skipping"
  fi
}

run_if_exists() 
{
  local outfile="$1"
  shift
  if [[ -e "${outfile}" ]]; then
    log "Running: $*"
    "$@"
  fi
}

check_exit() 
{
  local status=$1
  local msg=$2
  if [[ $status -ne 0 ]]; then
    log "ERROR: $msg"
    run_cleanup_scratch
    exit $status
  fi
}

verify_minimum_required_inputs()
{
  log "PRIMARY_DWI_NII : $(basename "${PRIMARY_DWI_NII:-}")"
  log "PRIMARY_DWI_BVAL: $(basename "${PRIMARY_DWI_BVAL:-}")"
  log "PRIMARY_DWI_BVEC: $(basename "${PRIMARY_DWI_BVEC:-}")"
  log "PRIMARY_DWI_JSON: $(basename "${PRIMARY_DWI_JSON:-}")"
  log "RPE_DWI_NII    : $(basename "${RPE_DWI_NII:-}")"
  log "RPE_BVAL      : $(basename "${RPE_BVAL:-}")"
  log "RPE_BVEC      : $(basename "${RPE_BVEC:-}")"
  log "RPE_JSON      : $(basename "${RPE_JSON:-}")"
  log "RPE_B0_1      : $(basename "${RPE_B0_1:-}")"
  log "RPE_B0_2      : $(basename "${RPE_B0_2:-}")"
  log "RPE_B0_1_JSON  : $(basename "${RPE_B0_1_JSON:-}")"
  log "RPE_B0_2_JSON  : $(basename "${RPE_B0_2_JSON:-}")"
  log "RPE_MIF       : $(basename "${RPE_MIF:-}")"

  if [[ -z "${PRIMARY_DWI_NII}" || -z "${PRIMARY_DWI_BVAL}" || \
        -z "${PRIMARY_DWI_BVEC}" || -z "${PRIMARY_DWI_JSON}" ]]; then
    log "ERROR: required input [nii.gz + json + bvec + bval] not found"
    log "Please check input directory: ${IN_DIR}/${SUB_ID}/${DWI_PFX}*"
    exit 1
  fi
}

#-------------------------------------------------------------------------------
### AUTO-DETECTION FUNCTIONS ###
#-------------------------------------------------------------------------------

detect_dti_files() {
  pushd "${IN_DIR}/${SUB_ID}" > /dev/null

  ### exit if more than two full DWI acquisitions are found ###
  dti_files=(${DWI_PFX}*.bvec)
  if (( ${#dti_files[@]} > 2 )); then
    log "ERROR: More than two DWI .nii.gz files found â€” unknown scenario"
    log "Please check input directory: ${IN_DIR}/${SUB_ID}/${DWI_PFX}*.nii.gz"
    exit 1
  fi

  ### first look for b0_1 and b0_2 ###
  RPE_B0_1=$(ls *b0_1*.nii.gz 2>/dev/null | head -n1)
  RPE_B0_2=$(ls *b0_2*.nii.gz 2>/dev/null | head -n1)
  if [[ -n "${RPE_B0_1}" && -n "${RPE_B0_2}" ]]; then
    RPE_B0_1_JSON="${RPE_B0_1%.nii.gz}.json"
    RPE_B0_2_JSON="${RPE_B0_2%.nii.gz}.json"
    if [[ -f "${RPE_B0_1}" && -f "${RPE_B0_1_JSON}" && -f "${RPE_B0_2}" && -f "${RPE_B0_2_JSON}" ]]; then
      local pe_b0_1=$(jq -r '.PhaseEncodingDirection' "${RPE_B0_1_JSON}")
      local pe_b0_2=$(jq -r '.PhaseEncodingDirection' "${RPE_B0_2_JSON}")
      log "RPE b0_1 PE: ${pe_b0_1}"
      log "RPE b0_2 PE: ${pe_b0_2}"
      if [[ "${pe_b0_1}" != "${pe_b0_2}" ]]; then
        log "ERROR: RPE b0_1 and b0_2 have different PhaseEncodingDirection"
        exit 1
      fi
      log "Found two RPE b0 images, generating combined rpe.mif"
      RPE_MIF="${OUT_DIR}/${SUB_ID}/dwi_detected_combined_rpe.mif"
      run_if_missing "${RPE_MIF}" \
        run_create_combined_rpe "${RPE_B0_1}" "${RPE_B0_1_JSON}" "${RPE_B0_2}" "${RPE_B0_2_JSON}" "${RPE_MIF}"
    fi
  fi

  ### then look for primary DTI scan ###
  for nii in ${DWI_PFX}*.nii.gz; do
    base="${nii%.nii.gz}"
    bval="${base}.bval"
    bvec="${base}.bvec"
    json="${base}.json"
    if [[ -f "${nii}" && -f "${bval}" && -f "${bvec}" && -f "${json}" ]]; then
      if ! jq -e 'has("PhaseEncodingDirection")' "$json" > /dev/null; then
        log "ERROR: PhaseEncodingDirection missing in ${json}"
        log "Please add first, inferring from PhaseEncodingAxis + ImageOrientationPatientDICOM"
        exit 1
      fi
      PRIMARY_DWI_NII="${nii}"
      PRIMARY_DWI_BVAL="${bval}"
      PRIMARY_DWI_BVEC="${bvec}"
      PRIMARY_DWI_JSON="${json}"
      break  # stop searching after finding the first DWI scan
    fi
  done

  ### if no rpe.mif (from b0_1/b0_2), look for second full DWI scan ###
  # TODO: implement
  if [[ -z "${RPE_MIF}" ]]; then
    for nii in ${DWI_PFX}*.nii.gz; do
      [[ "$nii" == "${PRIMARY_DWI_NII}" ]] && continue
      base="${nii%.nii.gz}"
      bval="${base}.bval"
      bvec="${base}.bvec"
      json="${base}.json"
      if [[ -f "${nii}" && -f "${bval}" && -f "${bvec}" && -f "${json}" ]]; then
        log "Found second DWI scan: ERROR -- not yet implemented"
        exit 1
      fi
    done
  fi

  ### if still no rpe.mif, look for single RPE volume with json only ###
  if [[ -z "${RPE_MIF}" ]]; then
    for nii in *.nii.gz; do
      [[ "$nii" == "${PRIMARY_DWI_NII}" ]] && continue
      base="${nii%.nii.gz}"
      bval="${base}.bval"
      bvec="${base}.bvec"
      json="${base}.json"
      if [[ -f "${json}" && ! -f "${bval}" && ! -f "${bvec}" ]]; then
        log "Found single-volume B0"
        RPE_DWI_NII="${nii}"
        RPE_MIF="${OUT_DIR}/${SUB_ID}/dwi_detected_rpe.mif"
        run_create_rpe "${nii}" "${json}" "${RPE_MIF}"
        break
      fi
    done
  fi

  ### finalize full paths for the non-empty variables ###
  for var in PRIMARY_DWI_NII PRIMARY_DWI_BVAL PRIMARY_DWI_BVEC PRIMARY_DWI_JSON \
             RPE_B0_1 RPE_B0_2 RPE_B0_1_JSON RPE_B0_2_JSON; do
    val="${!var}"
    [[ -n "$val" ]] && eval "$var=\"${IN_DIR}/${SUB_ID}/$val\""
  done

  ### log whether RPE data was found ###
  [[ -n "${RPE_MIF}" ]] && log "RPE data: ${RPE_MIF}" || log "No RPE data found"

  popd > /dev/null
}

detect_slice_orientation_from_json() 
{
  local json_file="${PRIMARY_DWI_JSON}"

  if [[ ! -f "${json_file}" ]]; then
    return
  fi

  local vals
  vals=($(jq -r '.ImageOrientationPatientDICOM[]?' "${json_file}"))

  if [[ "${#vals[@]}" -ne 6 ]]; then
    return
  fi

  local Xx="${vals[0]}" Xy="${vals[1]}" Xz="${vals[2]}"
  local Yx="${vals[3]}" Yy="${vals[4]}" Yz="${vals[5]}"

  local Sx Sy Sz
  Sx=$(echo "${Xy} * ${Yz} - ${Xz} * ${Yy}" | bc -l)
  Sy=$(echo "${Xz} * ${Yx} - ${Xx} * ${Yz}" | bc -l)
  Sz=$(echo "${Xx} * ${Yy} - ${Xy} * ${Yx}" | bc -l)

  local abs_Sx abs_Sy abs_Sz
  abs_Sx=$(echo "${Sx#-}" | bc -l)
  abs_Sy=$(echo "${Sy#-}" | bc -l)
  abs_Sz=$(echo "${Sz#-}" | bc -l)

  if (( $(echo "${abs_Sz} >= ${abs_Sx} && ${abs_Sz} >= ${abs_Sy}" | bc -l) )); then
    SLICE_OR="0,1"  # axial
  elif (( $(echo "${abs_Sy} >= ${abs_Sx} && ${abs_Sy} >= ${abs_Sz}" | bc -l) )); then
    SLICE_OR="0,2"  # coronal
  else
    SLICE_OR="1,2" # sagittal
  fi
  
  if [[ -z "${SLICE_OR}" ]]; then
    log "ERROR: Unable to determine slice orientation from JSON"
    exit 1
  fi
  
  log "Slice orientation detected: ${SLICE_OR}"

  if [[ "${SLICE_OR}" != "0,1" ]]; then
    log "WARNING: Slice orientation is not axial (0,1). Detected: ${SLICE_OR}"
  fi
}

#-------------------------------------------------------------------------------
### QC FUNCTIONS ###
#-------------------------------------------------------------------------------

run_screen_grab_image()
{
  ### generate png of "input" with "output_pnf_pfx" in QC_DIR with optional "overlay" ###
  local input="$1"
  local output_png_pfx="$2"
  local overlay="$3"
 
  local gui_runner=""
  if [[ "${CC_CLUSTER}" == "cedar" || "${CC_CLUSTER}" == "narval" ]]; then
    gui_runner="xvfb-run --auto-servernum --server-num=1000"
  fi

  local cmd="${gui_runner} mrview ${input}"
  if [[ -n "${overlay}" ]]; then
    cmd+=" -overlay.load ${overlay} -overlay.opacity 0.5 -overlay.colourmap 1"
  fi
  cmd+=" -mode 2 -capture.prefix ${output_png_pfx}_ -capture.folder ${QC_DIR} -capture.grab -exit"

  if [[ ! -e "${QC_DIR}/${output_png_pfx}_0000.png" && -e "${input}" && ( -z "${overlay}" || -e "${overlay}" ) ]]; then
    log "Running: ${cmd}"
    bash -c "${cmd}"
  else
    log "Skipping QC image generation: output already exists or input/overlay missing"
  fi
}

run_qc_residual() 
{
  ### generate residual image "output" from "original" and "processed" images ###
  ### save png of residual image with "png_pfx" in QC_DIR ###
  local original=$1
  local processed=$2
  local output=$3
  local png_pfx=$4

  ### generate residual image ###
  run_if_missing ${output} \
    mrcalc "${original}" "${processed}" -subtract ${output} -force -nthreads "${THREADS}"

  ### generate png of residual image ###
  run_screen_grab_image "${output}" "${png_pfx}"
}

#-------------------------------------------------------------------------------
### PROCESSING FUNCTIONS ###
#-------------------------------------------------------------------------------
# - "pipe_" : pipeline functions that operate on staged filenames
# - "run_"  : utility functions used by pipe functions with filenames as arguments

run_dwidenoise() 
{
  local input="$1"
  local output="$2"
  
  run_if_missing "${output}" \
    dwidenoise "${input}" "${output}" -nthreads "${THREADS}"
  check_exit $? "dwidenoise failed for ${input}"
}

run_degibbs() 
{
  local input="$1"
  local output="$2"

  run_if_missing "${output}" \
    mrdegibbs "${input}" "${output}" -nthreads "${THREADS}" -axes ${SLICE_OR}
  check_exit $? "mrdegibbs failed for ${input}"
}

run_create_rpe()
{
  local nii="$1"
  local json="$2"
  local out_mif="$3"

  if [[ ${DRY_RUN} -eq 1 ]]; then
    log "Skipping RPE creation for test run"
    return
  fi

  run_if_missing "${out_mif}" \
    mrconvert "${nii}" "${out_mif}" -json_import "${json}" -quiet -force
  check_exit $? "mrconvert failed for ${nii}"
}

run_create_combined_rpe() 
{
  local b0_1="$1"
  local b0_1_json="$2"
  local b0_2="$3"
  local b0_2_json="$4"
  local out_mif="$5"

  if [[ ${DRY_RUN} -eq 1 ]]; then
    log "Skipping RPE creation for test run"
    return
  fi

  if [[ -f "${out_mif}" ]]; then
    log "RPE already exists, skipping creation"
    exit 0
  fi

  local tmp1=$(mktemp "$(pwd)/tmp_rpe_$(date +%Y%m%d)_XXXXXX.mif")
  local tmp2=$(mktemp "$(pwd)/tmp_rpe_$(date +%Y%m%d)_XXXXXX.mif")

  mrconvert "${b0_1}" "${tmp1}" -json_import "${b0_1_json}" -quiet -force
  mrconvert "${b0_2}" "${tmp2}" -json_import "${b0_2_json}" -quiet -force
  mrcat "${tmp1}" "${tmp2}" -axis 3 "${out_mif}" -quiet -force
  check_exit $? "create combined rpe failed"

  rm -f "${tmp1}" "${tmp2}"
}


pipe_mrconvert()
{
  run_if_missing "${OUT_DIR_ROOT}/dwi.mif" \
    mrconvert ${PRIMARY_DWI_NII} "${OUT_DIR_ROOT}/dwi.mif" \
      -fslgrad ${PRIMARY_DWI_BVEC} ${PRIMARY_DWI_BVAL} \
      -json_import ${PRIMARY_DWI_JSON} \
      -nthreads "${THREADS}"
   check_exit $? "mrconvert failed for ${PRIMARY_DWI_NII}"
}

pipe_quantatitive_eddy_qc()
{
  local eddy_dir="${OUT_DIR_ROOT}/eddyqc_dir"
  local input_dwi="${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy.mif"
  local output_csv="${QC_DIR}/${SUB_ID}_eddy_qc_quantitative.csv"

  local file_movement_rms="${eddy_dir}/eddy_movement_rms"
  local file_restricted_movement_rms="${eddy_dir}/eddy_restricted_movement_rms"
  local file_outlier_map="${eddy_dir}/eddy_outlier_map"

  ### extract mean absolute motion (second column) ###
  mean_abs_motion=$(awk '{sum+=$2} END {if (NR>0) print sum/NR; else print "NA"}' "$file_movement_rms")

  ### extract mean relative motion (second column) ###
  mean_rel_motion=$(awk '{sum+=$2} END {if (NR>0) print sum/NR; else print "NA"}' "$file_restricted_movement_rms")

  ### calculate percent outlier slices ###
  n_outlier_slices=$(wc -l < "$file_outlier_map")

  n_volumes=$(mrinfo "$input_dwi" -size | awk '{print $4}')
  n_slices=$(mrinfo "$input_dwi" -size | awk '{print $3}')
  total_slices=$(( n_volumes * n_slices ))

  if [[ $total_slices -gt 0 ]]; then
    percent_outliers=$(awk -v outliers="$n_outlier_slices" -v total="$total_slices" 'BEGIN {print 100*outliers/total}')
  else
    percent_outliers="NA"
  fi

  ### write output CSV ###
  {
    echo "MeanAbsMotion,MeanRelMotion,PercentOutliers"
    echo "${mean_abs_motion},${mean_rel_motion},${percent_outliers}"
  } > "$output_csv"
}

pipe_dwi_preprocessing()
{
  log "preprocessing DWI"

  run_dwidenoise ${OUT_DIR_ROOT}/dwi.mif \
    ${OUT_DIR_ROOT}/dwi_denoised.mif
  
  run_degibbs ${OUT_DIR_ROOT}/dwi_denoised.mif \
    ${OUT_DIR_ROOT}/dwi_denoised_degibbs.mif

  run_qc_residual ${OUT_DIR_ROOT}/dwi.mif \
    ${OUT_DIR_ROOT}/dwi_denoised.mif \
    ${OUT_DIR_ROOT}/dwi_denoised_residuals.mif \
    ${SUB_ID}_dwi_denoised_residuals
    
  run_qc_residual ${OUT_DIR_ROOT}/dwi_denoised.mif \
    ${OUT_DIR_ROOT}/dwi_denoised_degibbs.mif \
    ${OUT_DIR_ROOT}/dwi_denoised_degibbs_residuals.mif \
    ${SUB_ID}_dwi_denoised_degibbs_residuals
}

pipe_rpe_preprocessing()
{
  ### preprocess rpe, if available (scenario 2) ###
  if [[ -f "${RPE_MIF}" && -z "${RPE_BVEC}" ]]; then
    log "preprocessing RPE B0"
    log "denoising skipped for rpe (scenario 2)"
    run_degibbs ${RPE_MIF} \
      "${OUT_DIR_ROOT}/rpe_denoised_degibbs.mif"
  fi

  ### TODO: implement scenario 3 ###
}

pipe_dwipreproc()
{
  local input=${OUT_DIR_ROOT}/dwi_denoised_degibbs.mif
  local output=${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy.mif
  local scratch_dir=${TMP_SCRATCH_DIR}/tmp_${SUB_ID}_$(date +%Y%m%d_%H%M%S)

  ### run dwifslpreproc by scenario ###
  if [[ -f "${RPE_MIF}" && -n "${RPE_BVEC}" ]]; then
  
    # scenario 3
    log "TODO: implement scenario 3 -- scenario 2 should work"
    exit 1

  elif [[ -f "${RPE_MIF}" && -z "${RPE_BVEC}" && -z ${RPE_B0_1} ]]; then
    # scenario 2a
    log "Running dwifslpreproc for DWI RPE (scenario 2a)"
    local rpe=${OUT_DIR_ROOT}/rpe_denoised_degibbs.mif
    run_if_missing "${output}" \
      dwifslpreproc ${input} ${output} \
        -rpe_header \
        -se_epi ${rpe} -align_seepi \
        -eddy_options "--slm=linear --repol --data_is_shelled --verbose --estimate_move_by_susceptibility" \
        -eddyqc_all ${OUT_DIR_ROOT}/eddyqc_dir \
        -info \
        -scratch ${scratch_dir} \
        -nthreads "${THREADS}" \
        -force
    check_exit $? "dwifslpreproc failed for ${input}"

  elif [[ -f "${RPE_MIF}" && -z "${RPE_BVEC}" && -n "${RPE_B0_1}" ]]; then
    # scenario 2b
    log "Running dwifslpreproc for DWI RPE-PAIR (scenario 2b)"
    local rpe=${OUT_DIR_ROOT}/rpe_denoised_degibbs.mif
    local pe=$(jq -r '.PhaseEncodingDirection' "${RPE_B0_1_JSON}")
    run_if_missing "${output}" \
      dwifslpreproc ${input} ${output} \
        -rpe_pair -pe_dir=${pe} \
        -se_epi ${rpe} -align_seepi \
        -eddy_options "--slm=linear --repol --data_is_shelled --verbose --estimate_move_by_susceptibility" \
        -eddyqc_all ${OUT_DIR_ROOT}/eddyqc_dir \
        -info \
        -scratch ${scratch_dir} \
        -nthreads "${THREADS}" \
        -force
    check_exit $? "dwifslpreproc failed for ${input}"
    
  else
    # scenario 1
    log "Running dwifslpreproc for DWI no RPE acquisition (scenario 1)"
    run_if_missing ${output} \
      dwifslpreproc ${input} ${output} \
        -rpe_header \
        -eddy_options "--slm=linear --repol --data_is_shelled --verbose" \
        -eddyqc_all ${OUT_DIR_ROOT}/eddyqc_dir \
        -info \
        -scratch ${scratch_dir} \
        -nthreads "${THREADS}" \
        -force
    check_exit $? "dwifslpreproc failed for ${input}"
  fi

  ### qc dwifslpreproc ###
  cp ${OUT_DIR_ROOT}/eddyqc_dir/quad/qc.pdf ${QC_DIR}/${SUB_ID}_quad_eddy_qc.pdf
  pipe_quantatitive_eddy_qc
  run_screen_grab_image \
    ${output} \
    ${SUB_ID}_dwifslpreproc_qc \
    ${input}
}

pipe_bias_correct()
{
  local input=${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy.mif
  local output=${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy_N4.mif
  local mask=${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy_mask.mif

  run_if_missing ${mask} \
    dwi2mask "${input}" "${mask}" -nthreads "${THREADS}"

  run_if_missing "${output}" \
    dwibiascorrect ants "${input}" "${output}" \
    -mask "${mask}" \
    -nthreads "${THREADS}" \
    -scratch "${TMP_SCRATCH_DIR}" 
  check_exit $? "dwibiascorrect failed for ${input}"
}

pipe_upsample()
{
  local input=${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy_N4.mif
  local output=${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy_N4_${REGRID_SIZE_FORMATTED}.mif
  
  ### upsample if needed, otherwise symlink ###
  current_spacing=$(mrinfo "${input}" -spacing | cut -d' ' -f 1-3 | sed "s/ /,/g")
  if [[ "$current_spacing" != "${REGRID_SIZE}" ]]; then
    run_if_missing "${output}" \
      mrgrid "${input}" regrid -voxel ${REGRID_SIZE} "${output}" \
        -interp linear -force -nthreads "${THREADS}"
      check_exit $? "mrgrid failed for ${input}"
  else
    log "Regrid size ${REGRID_SIZE} already matches input spacing ${current_spacing}"
    run_if_missing "${output}" \
       ln -s "$(realpath "${input}")" "${output}"
  fi
}

pipe_tensor_fit()
{
  local input=${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy_N4_${REGRID_SIZE_FORMATTED}.mif 
  local output_dir="${OUT_DIR_ROOT}/${REGRID_SIZE_FORMATTED}_tensor_fit"
  
  mkdir -p "${output_dir}"

  ### convert to nifti for dtifit ###
  local dwi_nii="${output_dir}/dwi.nii.gz"
  local bvec="${output_dir}/dwi.bvecs"
  local bval="${output_dir}/dwi.bvals"
  local mrtrix_grad="${output_dir}/dwi_grad_table"
  run_if_missing "${dwi_nii}" \
    mrconvert "${input}" "${dwi_nii}" \
      -export_grad_fsl "${bvec}" "${bval}" \
      -export_grad_mrtrix "${mrtrix_grad}" \
      -nthreads "${THREADS}" -info

  ### create mask for tensor fit ###
  local mask="${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy_N4_${REGRID_SIZE_FORMATTED}_mask.nii.gz"
  bet "${dwi_nii}" ${mask} -f 0.3
  
  ### run DTI fitting ###
  run_if_missing "${output_dir}/dwi_FA.nii.gz" \
    dtifit \
      --data="${dwi_nii}" \
      --out="${output_dir}/dwi" \
      --mask="${mask}" \
      --bvecs="${bvec}" \
      --bvals="${bval}" \
      --save_tensor
  check_exit $? "dtifit failed for ${dwi_nii}"

  ### generate png of FA image and save to QC_DIR ###
  run_screen_grab_image ${output_dir}/dwi_FA.nii.gz \
    ${SUB_ID}_dwi_FA_${REGRID_SIZE_FORMATTED} 
}

pipe_generate_additional_dti_maps()
{
  local tensor_dir="${OUT_DIR_ROOT}/${REGRID_SIZE_FORMATTED}_tensor_fit"
  local L2="${tensor_dir}/dwi_L2.nii.gz"
  local L3="${tensor_dir}/dwi_L3.nii.gz"

  log "Generating additional DTI scalar maps in ${tensor_dir}"

  ### axial diffusivity (AD = L1) ###
  
  ### Radial diffusivity (RD = (L2 + L3)/2) ###
  run_if_missing ${tensor_dir}/dwi_RD.nii.gz \
    fslmaths "${L2}" -add "${L3}" -div 2 "${tensor_dir}/dwi_RD.nii.gz"
}

pipe_tbss()
{
  log "Running TBSS pipeline"

  local tensor_dir="${OUT_DIR_ROOT}/${REGRID_SIZE_FORMATTED}_tensor_fit"
  local fa_image="${tensor_dir}/dwi_FA.nii.gz"
  local tensor_image="${tensor_dir}/dwi_tensor.nii.gz"
  local tbss_dir="${OUT_DIR_ROOT}/${REGRID_SIZE_FORMATTED}_tbss"
  
  mkdir -p "${tbss_dir}/FA"
  mkdir -p "${tbss_dir}/tensor"

  pushd "${tbss_dir}" > /dev/null

  ### Step 0: Prepare FA and tensor images for TBSS ###
  cp "${fa_image}" "FA/dwi_FA.nii.gz"
  cp "${tensor_image}" "tensor/dwi.nii.gz"

  ### Step 1: Preprocess FA (reorient and mask) ###
  log "Step 1: TBSS preprocessing"
  run_if_missing "FA/slicesdir/index.html" \
    tbss_1_preproc FA
  check_exit $? "tbss_1_preproc failed for ${fa_image}"

  ### Step 2: Register FA to FMRIB58 template ###
  log "Step 2: TBSS registration"
  run_if_missing "FA/dwi_FA_to_target_warp.nii.gz" \
    tbss_2_reg -T
  check_exit $? "tbss_2_reg failed for ${fa_image}"

  ### Step 3: Post-registration and skeleton ###
  log "Step 3: TBSS post-registration"
  run_if_missing "stats/all_FA.nii.gz" \
    tbss_3_postreg -S
  check_exit $? "tbss_3_postreg failed for ${fa_image}"

  ### Step 4: Threshold mean FA skeleton ###
  log "Step 4: TBSS skeleton thresholding"
  run_if_missing "stats/mean_FA_skeleton_mask_dst.nii.gz" \
    tbss_4_prestats 0.2
  check_exit $? "tbss_4_prestats failed for ${fa_image}"

  ### Step 5: Non-FA projection (tensor) ###
  log "Step 5: TBSS non-FA projection (tensor)"
  run_if_missing "FA/dwi_to_target_tensor.nii.gz" \
    tbss_non_FA tensor
  check_exit $? "tbss_non_FA failed for ${tensor_image}"

  ### QC snapshot ###
  run_screen_grab_image "${tbss_dir}/stats/mean_FA_skeleton.nii.gz" \
    ${SUB_ID}_tbss_skeleton_${REGRID_SIZE_FORMATTED} 

  popd > /dev/null
}

pipe_alps_extract()
{
 log "Running ALPS extraction"

  local tbss_tensor="${OUT_DIR_ROOT}/${REGRID_SIZE_FORMATTED}_tbss/FA/dwi_to_target_tensor.nii.gz"
  local alps_dir="${OUT_DIR_ROOT}/${REGRID_SIZE_FORMATTED}_alps"
  local tensor_dir="${alps_dir}/tensor"
  local comp_nifti="${tensor_dir}/tensor_components.nii.gz"
  local tensor_csv="${alps_dir}/alps_tensor_components.csv"
  local index_csv="${alps_dir}/alps_indices.csv"

  if [[ -f ${index_csv} ]]; then
    log "ALPS indices already exist, skipping extraction"
    return
  fi

  mkdir -p "${tensor_dir}"

  # Disassemble tensor components
  cp "${tbss_tensor}" "${tensor_dir}/tensor.nii.gz"
  ImageMath 4 "${comp_nifti}" TimeSeriesDisassemble "${tensor_dir}/tensor.nii.gz"

  # Rename components
  local i=0
  for comp in xx xy xz yy yz zz; do
    mv "${tensor_dir}/tensor_components100${i}.nii.gz" "${tensor_dir}/D${comp}.nii.gz"
    ((i++))
  done

  # Extract mean values for each ROI
  declare -A vals
  for hemi in L R; do
    vals[${hemi}_Dxproj]=$(fslstats "${tensor_dir}/Dxx.nii.gz" -k "${ALPS_ROI_DIR}/${hemi}_hemi_proj.nii.gz" -m)
    vals[${hemi}_Dxassoc]=$(fslstats "${tensor_dir}/Dxx.nii.gz" -k "${ALPS_ROI_DIR}/${hemi}_hemi_assoc.nii.gz" -m)
    vals[${hemi}_Dyproj]=$(fslstats "${tensor_dir}/Dyy.nii.gz" -k "${ALPS_ROI_DIR}/${hemi}_hemi_proj.nii.gz" -m)
    vals[${hemi}_Dzassoc]=$(fslstats "${tensor_dir}/Dzz.nii.gz" -k "${ALPS_ROI_DIR}/${hemi}_hemi_assoc.nii.gz" -m)
  done

  # Compute ALPS indices
  L_ALPS=$(echo "scale=4; (${vals[L_Dxproj]} + ${vals[L_Dxassoc]}) / (${vals[L_Dyproj]} + ${vals[L_Dzassoc]})" | bc)
  R_ALPS=$(echo "scale=4; (${vals[R_Dxproj]} + ${vals[R_Dxassoc]}) / (${vals[R_Dyproj]} + ${vals[R_Dzassoc]})" | bc)

  # Save tensor values CSV
  {
    echo "Subject,L_Dxproj,L_Dxassoc,L_Dyproj,L_Dzassoc,R_Dxproj,R_Dxassoc,R_Dyproj,R_Dzassoc"
    echo "${SUB_ID},${vals[L_Dxproj]},${vals[L_Dxassoc]},${vals[L_Dyproj]},${vals[L_Dzassoc]},"\
        "${vals[R_Dxproj]},${vals[R_Dxassoc]},${vals[R_Dyproj]},${vals[R_Dzassoc]}"
  } > "${tensor_csv}"

  # Save ALPS index CSV
  {
    echo "Subject,L_ALPS,R_ALPS"
    echo "${SUB_ID},${L_ALPS},${R_ALPS}"
  } > "${index_csv}"

  log "ALPS extraction complete: ${index_csv}"

  ### check exit that L_ALPS and R_ALPS are not empty ###
  if [[ -z "${L_ALPS}" || -z "${R_ALPS}" ]]; then
    log "ERROR: ALPS indices are empty"
    exit 1
  fi
}

pipe_calculate_free_water_fraction() 
{
  local dwi="${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy_N4_${REGRID_SIZE_FORMATTED}.mif"
  local mask="${OUT_DIR_ROOT}/dwi_denoised_degibbs_eddy_N4_${REGRID_SIZE_FORMATTED}_mask.nii.gz"
  local output_dir="${OUT_DIR_ROOT}/${REGRID_SIZE_FORMATTED}_free_water_fraction"
  local bval="${OUT_DIR_ROOT}/${REGRID_SIZE_FORMATTED}_tensor_fit/dwi.bvals"

  n_shells=$(cat ${bval} | tr ' ' '\n' | awk '$1 > 50' | sort -n | uniq | wc -l)

  log "Number of shells: ${n_shells}"
  if [[ ${n_shells} -lt 2 ]]; then
    log "WARNING: Single-shell data â€” FW_fraction.mif is semi-quantitative only"
  fi

  mkdir -p "${output_dir}"

  log "Running tissue response estimation (dhollander)..."
  run_if_missing "${output_dir}/wm.txt" \
    dwi2response dhollander "$dwi" \
      "${output_dir}/wm.txt" "${output_dir}/gm.txt" "${output_dir}/csf.txt" \
      -mask "$mask" -nthreads "$THREADS" -force -scratch "${TMP_SCRATCH_DIR}"
  check_exit $? "dwi2response failed for ${dwi}"

  log "Running dwi2fod"
  run_if_missing "${output_dir}/wmfod.mif" \
    dwi2fod msmt_csd "$dwi" \
      "${output_dir}/wm.txt" "${output_dir}/wmfod.mif" \
      "${output_dir}/gm.txt" "${output_dir}/gmfod.mif" \
      "${output_dir}/csf.txt" "${output_dir}/csffod.mif" \
      -mask "$mask" -nthreads "$THREADS" -force
  check_exit $? "dwi2fod failed for ${dwi}"

  ### visual QC of wmfod ###
  run_screen_grab_image "${output_dir}/wmfod.mif" \
    "${SUB_ID}_wmfod_${REGRID_SIZE_FORMATTED}"

  ### calculate FW ###
  log "Computing total signal and free water fraction..."
  run_if_missing "${output_dir}/total.mif" \
    mrcalc "${output_dir}/wmfod.mif" "${output_dir}/gmfod.mif" -add \
      "${output_dir}/csffod.mif" -add "${output_dir}/total.mif" -force

  run_if_missing "${output_dir}/FW_fraction.mif" \
    mrcalc "${output_dir}/csffod.mif" "${output_dir}/total.mif" -div \
      "${output_dir}/FW_fraction.mif" -force

  run_screen_grab_image "${output_dir}/FW_fraction.mif" \
    "${SUB_ID}_FW_fraction_${REGRID_SIZE_FORMATTED}"
}

#-------------------------------------------------------------------------------
### CLEANUP FUNCTIONS ###
#-------------------------------------------------------------------------------

run_cleanup_scratch()
{
  ### delete TMP_SCRATCH_DIR ###
  if [[ -d "${TMP_SCRATCH_DIR}" ]]; then
    log "Deleting scratch directory: ${TMP_SCRATCH_DIR}"
    rm -rf "${TMP_SCRATCH_DIR}"
  fi
}

pipe_cleanup()
{
  run_cleanup_scratch

  pushd "${OUT_DIR_ROOT}" > /dev/null

  log "Cleaning up temporary files"
  rm -f dwi_denoised_degibbs_eddy_mask.mif 
  rm -f dwi_denoised_degibbs_eddy.mif  
  rm -f dwi_denoised_degibbs.mif
  rm -f dwi_denoised*_residuals.mif
  rm -f dwi.mif
  rm -f dwi_denoised.mif
  
  rm -f ${REGRID_SIZE_FORMATTED}_alps/tensor/tensor.nii.gz
  rm -f ${REGRID_SIZE_FORMATTED}_tensor_fit/dwi.nii.gz
  rm -f ${REGRID_SIZE_FORMATTED}_tensor_fit/dwi_grad_table
  rm -f ${REGRID_SIZE_FORMATTED}_tensor_fit/dwi.bvecs
  rm -f ${REGRID_SIZE_FORMATTED}_tensor_fit/dwi.bvals

  rm -f ${REGRID_SIZE_FORMATTED}_tbss/FA/dwi_FA_to_target.nii.gz
  rm -f ${REGRID_SIZE_FORMATTED}_tbss/FA/dwi_FA_to_target.mat
  rm -f ${REGRID_SIZE_FORMATTED}_tbss/FA/dwi_FA_to_target.log
  rm -f ${REGRID_SIZE_FORMATTED}_tbss/FA/dwi_FA_to_target_warp.nii.gz
  rm -f ${REGRID_SIZE_FORMATTED}_tbss/FA/dwi_to_target_tensor.nii.gz
  rm -f ${REGRID_SIZE_FORMATTED}_tbss/FA/dwi_FA_to_target_warp.msf
  rm -f ${REGRID_SIZE_FORMATTED}_tbss/FA/best.msf
  rm -rf ${REGRID_SIZE_FORMATTED}_tbss/FA/slicesdir
  rm -rf ${REGRID_SIZE_FORMATTED}_tbss/FA/tbss_logs
  rm -rf ${REGRID_SIZE_FORMATTED}_tbss/origdata
  rm -f ${REGRID_SIZE_FORMATTED}_tbss/FA/target.nii.gz

  rm -f eddyqc_dir/eddy_mask.nii
  rm -f eddyqc_dir/eddy_outlier_free_data.nii.gz

  popd > /dev/null
}

#-------------------------------------------------------------------------------
### MAIN ###
#-------------------------------------------------------------------------------

main() 
{
  ### start pipeline ###
  parse_arguments "$@"
  setup_environment
  setup_directories  
  local start_time=$(date +%s)
  log "----------------------------------------"
  log "DWI pipeline STARTED for ${SUB_ID}: time $(date '+%F %T')"

  ### try to detect DTI files ###
  log "Auto-detecting DWI files..."
  detect_dti_files
  detect_slice_orientation_from_json
  
  ### prep for preprocessing ###
  verify_minimum_required_inputs

  ### halt if DRY_RUN is set ###
  if [[ ${DRY_RUN} -eq 1 ]]; then
    log "DRY_RUN is set to 1, skipping pipeline execution"
    run_cleanup_scratch
    return
  fi

  ### start pipeline processing steps ###
  pipe_mrconvert

  ### run initial dwi preprocessing ###
  pipe_dwi_preprocessing

  ## run initial rpe preprocessing (runs only when rpe data is detected) ###
  pipe_rpe_preprocessing

  ### run core dwi preprocessing ###
  pipe_dwipreproc 

  ### run bias correction ###
  pipe_bias_correct 

  ### run upsampling ###
  pipe_upsample

  ### run tensor fitting ###
  pipe_tensor_fit

  ### run additional DTI maps generation ###
  pipe_generate_additional_dti_maps

  ### run tbss ###
  pipe_tbss

  ### run alps extraction ###
  pipe_alps_extract

  ### run single shell free water fraction calculation ###
  pipe_calculate_free_water_fraction

  ### end pipeline ###
  if [[ -f "${OUT_DIR_ROOT}/${REGRID_SIZE_FORMATTED}_free_water_fraction/FW_fraction.mif" ]]; then
     pipe_cleanup
  else
     run_cleanup_scratch
  fi
  local end_time=$(date +%s)
  local elapsed_seconds=$((end_time - start_time))
  local elapsed_minutes=$((elapsed_seconds / 60))
  local remaining_seconds=$((elapsed_seconds % 60))
  log "DWI pipeline COMPLETED for ${SUB_ID}: time $(date '+%F %T')"
  log "TOTAL PROCESSING TIME: ${elapsed_minutes} minutes ${remaining_seconds} seconds"
}

main "$@"

#-------------------------------------------------------------------------------

